{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Les techniques avancées de RAG\n"
      ],
      "metadata": {
        "id": "Xt43XhfmQWj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment variales"
      ],
      "metadata": {
        "id": "Ux1vi8cdOQEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## --> Add Necessary env variables here\n"
      ],
      "metadata": {
        "id": "z6jQmccrOb5N"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Préparation de l'environnement de travail"
      ],
      "metadata": {
        "id": "VXxRus_DcfE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "jOCCQxmYcqUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60df3b17-eef2-44e4-d878-2fcab65e0426"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 29 19:49:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General upgrade\n",
        "!apt-get update -y && apt-get upgrade -y"
      ],
      "metadata": {
        "id": "jVfdtApKcw2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffafffd-7257-4f4d-d5b2-813c197cda52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r            \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,801 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,067 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,040 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,561 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,741 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,350 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,254 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,587 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,750 kB]\n",
            "Fetched 32.5 MB in 5s (6,340 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn9-cuda-12 libcudnn9-dev-cuda-12 libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  base-files binutils binutils-common binutils-x86-64-linux-gnu\n",
            "  cuda-toolkit-12-config-common cuda-toolkit-config-common e2fsprogs\n",
            "  libbinutils libc-bin libcap2 libctf-nobfd0 libctf0 libext2fs2 libgnutls30\n",
            "  libldap-2.5-0 libpam-modules libpam-modules-bin libpam-runtime libpam0g\n",
            "  libperl5.34 libseccomp2 libss2 libtasn1-6 libudev1 libxslt1.1 linux-libc-dev\n",
            "  logsave openssl perl perl-base perl-modules-5.34 python3-pkg-resources\n",
            "32 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 19.5 MB of archives.\n",
            "After this operation, 304 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 base-files amd64 12ubuntu4.7 [61.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libperl5.34 amd64 5.34.0-3ubuntu1.4 [4,820 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl amd64 5.34.0-3ubuntu1.4 [232 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-base amd64 5.34.0-3ubuntu1.4 [1,759 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-config-common 12.9.79-1 [16.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 perl-modules-5.34 all 5.34.0-3ubuntu1.4 [2,977 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-bin amd64 2.35-0ubuntu3.10 [706 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam0g amd64 1.4.0-11ubuntu2.6 [59.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-modules-bin amd64 1.4.0-11ubuntu2.6 [37.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-modules amd64 1.4.0-11ubuntu2.6 [282 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logsave amd64 1.46.5-2ubuntu1.2 [10.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libext2fs2 amd64 1.46.5-2ubuntu1.2 [208 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 e2fsprogs amd64 1.46.5-2ubuntu1.2 [590 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcap2 amd64 1:2.44-1ubuntu0.22.04.2 [18.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpam-runtime all 1.4.0-11ubuntu2.6 [40.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pkg-resources all 68.1.2-2~jammy3 [216 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtasn1-6 amd64 4.18.0-4ubuntu0.1 [43.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgnutls30 amd64 3.7.3-4ubuntu1.6 [969 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libseccomp2 amd64 2.5.3-2ubuntu3~22.04.1 [47.4 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libss2 amd64 1.46.5-2ubuntu1.2 [12.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssl amd64 3.0.2-0ubuntu1.19 [1,186 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf0 amd64 2.38-4ubuntu2.8 [103 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf-nobfd0 amd64 2.38-4ubuntu2.8 [108 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.38-4ubuntu2.8 [2,324 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libbinutils amd64 2.38-4ubuntu2.8 [661 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils amd64 2.38-4ubuntu2.8 [3,196 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-common amd64 2.38-4ubuntu2.8 [223 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libldap-2.5-0 amd64 2.5.19+dfsg-0ubuntu0.22.04.1 [184 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxslt1.1 amd64 1.1.34-4ubuntu0.22.04.4 [165 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 linux-libc-dev amd64 5.15.0-142.152 [1,321 kB]\n",
            "Get:32 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-config-common 12.9.79-1 [16.6 kB]\n",
            "Fetched 19.5 MB in 4s (4,720 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../base-files_12ubuntu4.7_amd64.deb ...\n",
            "Unpacking base-files (12ubuntu4.7) over (12ubuntu4.6) ...\n",
            "Setting up base-files (12ubuntu4.7) ...\n",
            "Installing new version of config file /etc/issue ...\n",
            "Installing new version of config file /etc/issue.net ...\n",
            "Installing new version of config file /etc/lsb-release ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libperl5.34_5.34.0-3ubuntu1.4_amd64.deb ...\n",
            "Unpacking libperl5.34:amd64 (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n",
            "Preparing to unpack .../perl_5.34.0-3ubuntu1.4_amd64.deb ...\n",
            "Unpacking perl (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n",
            "Preparing to unpack .../perl-base_5.34.0-3ubuntu1.4_amd64.deb ...\n",
            "Unpacking perl-base (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n",
            "Setting up perl-base (5.34.0-3ubuntu1.4) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../perl-modules-5.34_5.34.0-3ubuntu1.4_all.deb ...\n",
            "Unpacking perl-modules-5.34 (5.34.0-3ubuntu1.4) over (5.34.0-3ubuntu1.3) ...\n",
            "Preparing to unpack .../libc-bin_2.35-0ubuntu3.10_amd64.deb ...\n",
            "Unpacking libc-bin (2.35-0ubuntu3.10) over (2.35-0ubuntu3.8) ...\n",
            "Setting up libc-bin (2.35-0ubuntu3.10) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam0g_1.4.0-11ubuntu2.6_amd64.deb ...\n",
            "Unpacking libpam0g:amd64 (1.4.0-11ubuntu2.6) over (1.4.0-11ubuntu2.4) ...\n",
            "Setting up libpam0g:amd64 (1.4.0-11ubuntu2.6) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-modules-bin_1.4.0-11ubuntu2.6_amd64.deb ...\n",
            "Unpacking libpam-modules-bin (1.4.0-11ubuntu2.6) over (1.4.0-11ubuntu2.4) ...\n",
            "Setting up libpam-modules-bin (1.4.0-11ubuntu2.6) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-modules_1.4.0-11ubuntu2.6_amd64.deb ...\n",
            "Unpacking libpam-modules:amd64 (1.4.0-11ubuntu2.6) over (1.4.0-11ubuntu2.4) ...\n",
            "Setting up libpam-modules:amd64 (1.4.0-11ubuntu2.6) ...\n",
            "Installing new version of config file /etc/security/namespace.init ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../logsave_1.46.5-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking logsave (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n",
            "Preparing to unpack .../libext2fs2_1.46.5-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking libext2fs2:amd64 (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n",
            "Setting up libext2fs2:amd64 (1.46.5-2ubuntu1.2) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../e2fsprogs_1.46.5-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking e2fsprogs (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n",
            "Preparing to unpack .../libcap2_1%3a2.44-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libcap2:amd64 (1:2.44-1ubuntu0.22.04.2) over (1:2.44-1ubuntu0.22.04.1) ...\n",
            "Setting up libcap2:amd64 (1:2.44-1ubuntu0.22.04.2) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-runtime_1.4.0-11ubuntu2.6_all.deb ...\n",
            "Unpacking libpam-runtime (1.4.0-11ubuntu2.6) over (1.4.0-11ubuntu2.4) ...\n",
            "Setting up libpam-runtime (1.4.0-11ubuntu2.6) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libtasn1-6_4.18.0-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtasn1-6:amd64 (4.18.0-4ubuntu0.1) over (4.18.0-4build1) ...\n",
            "Setting up libtasn1-6:amd64 (4.18.0-4ubuntu0.1) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libgnutls30_3.7.3-4ubuntu1.6_amd64.deb ...\n",
            "Unpacking libgnutls30:amd64 (3.7.3-4ubuntu1.6) over (3.7.3-4ubuntu1.5) ...\n",
            "Setting up libgnutls30:amd64 (3.7.3-4ubuntu1.6) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../libseccomp2_2.5.3-2ubuntu3~22.04.1_amd64.deb ...\n",
            "Unpacking libseccomp2:amd64 (2.5.3-2ubuntu3~22.04.1) over (2.5.3-2ubuntu2) ...\n",
            "Setting up libseccomp2:amd64 (2.5.3-2ubuntu3~22.04.1) ...\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libss2_1.46.5-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking libss2:amd64 (1.46.5-2ubuntu1.2) over (1.46.5-2ubuntu1.1) ...\n",
            "Preparing to unpack .../01-openssl_3.0.2-0ubuntu1.19_amd64.deb ...\n",
            "Unpacking openssl (3.0.2-0ubuntu1.19) over (3.0.2-0ubuntu1.16) ...\n",
            "Preparing to unpack .../02-libctf0_2.38-4ubuntu2.8_amd64.deb ...\n",
            "Unpacking libctf0:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../03-libctf-nobfd0_2.38-4ubuntu2.8_amd64.deb ...\n",
            "Unpacking libctf-nobfd0:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../04-binutils-x86-64-linux-gnu_2.38-4ubuntu2.8_amd64.deb ...\n",
            "Unpacking binutils-x86-64-linux-gnu (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../05-libbinutils_2.38-4ubuntu2.8_amd64.deb ...\n",
            "Unpacking libbinutils:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../06-binutils_2.38-4ubuntu2.8_amd64.deb ...\n",
            "Unpacking binutils (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../07-binutils-common_2.38-4ubuntu2.8_amd64.deb ...\n",
            "Unpacking binutils-common:amd64 (2.38-4ubuntu2.8) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../08-cuda-toolkit-12-config-common_12.9.79-1_all.deb ...\n",
            "Unpacking cuda-toolkit-12-config-common (12.9.79-1) over (12.5.82-1) ...\n",
            "Preparing to unpack .../09-cuda-toolkit-config-common_12.9.79-1_all.deb ...\n",
            "Unpacking cuda-toolkit-config-common (12.9.79-1) over (12.5.82-1) ...\n",
            "Preparing to unpack .../10-libldap-2.5-0_2.5.19+dfsg-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libldap-2.5-0:amd64 (2.5.19+dfsg-0ubuntu0.22.04.1) over (2.5.17+dfsg-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../11-libxslt1.1_1.1.34-4ubuntu0.22.04.4_amd64.deb ...\n",
            "Unpacking libxslt1.1:amd64 (1.1.34-4ubuntu0.22.04.4) over (1.1.34-4ubuntu0.22.04.3) ...\n",
            "Preparing to unpack .../12-linux-libc-dev_5.15.0-142.152_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (5.15.0-142.152) over (5.15.0-113.123) ...\n",
            "Preparing to unpack .../13-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.3) ...\n",
            "Setting up python3-pkg-resources (68.1.2-2~jammy3) ...\n",
            "Setting up cuda-toolkit-config-common (12.9.79-1) ...\n",
            "Setting up binutils-common:amd64 (2.38-4ubuntu2.8) ...\n",
            "Setting up linux-libc-dev:amd64 (5.15.0-142.152) ...\n",
            "Setting up libctf-nobfd0:amd64 (2.38-4ubuntu2.8) ...\n",
            "Setting up perl-modules-5.34 (5.34.0-3ubuntu1.4) ...\n",
            "Setting up libldap-2.5-0:amd64 (2.5.19+dfsg-0ubuntu0.22.04.1) ...\n",
            "Setting up libss2:amd64 (1.46.5-2ubuntu1.2) ...\n",
            "Setting up logsave (1.46.5-2ubuntu1.2) ...\n",
            "Setting up libxslt1.1:amd64 (1.1.34-4ubuntu0.22.04.4) ...\n",
            "Setting up libbinutils:amd64 (2.38-4ubuntu2.8) ...\n",
            "Setting up openssl (3.0.2-0ubuntu1.19) ...\n",
            "Setting up cuda-toolkit-12-config-common (12.9.79-1) ...\n",
            "Setting up libctf0:amd64 (2.38-4ubuntu2.8) ...\n",
            "Setting up libperl5.34:amd64 (5.34.0-3ubuntu1.4) ...\n",
            "Setting up e2fsprogs (1.46.5-2ubuntu1.2) ...\n",
            "Setting up perl (5.34.0-3ubuntu1.4) ...\n",
            "Setting up binutils-x86-64-linux-gnu (2.38-4ubuntu2.8) ...\n",
            "Setting up binutils (2.38-4ubuntu2.8) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.10) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch\n",
        "!pip install torch\n",
        "\n",
        "# Instal TensorFlow (colab GPU)\n",
        "!pip install tensorflow\n",
        "\n",
        "# Install Hugging Face Transformers, Datasets, et Diffusers\n",
        "!pip install transformers datasets diffusers accelerate huggingface_hub\n",
        "\n",
        "# Install langchain, langgraph\n",
        "!pip install langchain langchain_ollama langchain_community langchain-nomic langgraph langchain_openai langsmith\n",
        "\n",
        "!pip install ipywidgets tiktoken \"nomic[local]\" scikit-learn\n",
        "\n",
        "# Install other utils\n",
        "!pip install chromadb diskcache"
      ],
      "metadata": {
        "id": "Co2wFoB9cyq2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "389065c4-c6ff-4815-cce9-be0ea50ecabd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain_ollama\n",
            "  Downloading langchain_ollama-0.3.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-nomic\n",
            "  Downloading langchain_nomic-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.5.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting ollama<1.0.0,>=0.4.8 (from langchain_ollama)\n",
            "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting nomic<4.0.0,>=3.1.2 (from langchain-nomic)\n",
            "  Downloading nomic-3.5.3.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pillow<11.0.0,>=10.3.0 (from langchain-nomic)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting langgraph-checkpoint>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt>=0.5.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.91.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.18)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (8.2.1)\n",
            "Collecting jsonlines (from nomic<4.0.0,>=3.1.2->langchain-nomic)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting loguru (from nomic<4.0.0,>=3.1.2->langchain-nomic)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (13.9.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (4.67.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (18.1.0)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.10.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->nomic<4.0.0,>=3.1.2->langchain-nomic) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->nomic<4.0.0,>=3.1.2->langchain-nomic) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->nomic<4.0.0,>=3.1.2->langchain-nomic) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (1.17.0)\n",
            "Downloading langchain_ollama-0.3.3-py3-none-any.whl (21 kB)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_nomic-0.1.4-py3-none-any.whl (3.9 kB)\n",
            "Downloading langgraph-0.5.0-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.5.1-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
            "Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: nomic\n",
            "  Building wheel for nomic (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nomic: filename=nomic-3.5.3-py3-none-any.whl size=50596 sha256=8e5f4f113c14ed7354f8fbdbbba641b5a525187a99b90539f20c11577d51d88f\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/2c/65/1f8732a5e7c19c13b8274833431d936144b4343b6a8d0fb1dc\n",
            "Successfully built nomic\n",
            "Installing collected packages: python-dotenv, pillow, ormsgpack, mypy-extensions, marshmallow, loguru, jsonlines, httpx-sse, typing-inspect, pydantic-settings, ollama, nomic, langgraph-sdk, dataclasses-json, langgraph-checkpoint, langchain_openai, langchain_ollama, langchain-nomic, langgraph-prebuilt, langgraph, langchain_community\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 jsonlines-4.0.0 langchain-nomic-0.1.4 langchain_community-0.3.26 langchain_ollama-0.3.3 langchain_openai-0.3.27 langgraph-0.5.0 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.5.1 langgraph-sdk-0.1.72 loguru-0.7.3 marshmallow-3.26.1 mypy-extensions-1.1.0 nomic-3.5.3 ollama-0.5.1 ormsgpack-1.10.0 pillow-10.4.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "6d41623181224f2ebf79e390b8eda762"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nomic[local] in /usr/local/lib/python3.11/dist-packages (3.5.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (8.2.1)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (4.0.0)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (0.7.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (13.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (2.2.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (4.67.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (18.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (10.4.0)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.11/dist-packages (from nomic[local]) (2.10.1)\n",
            "Collecting gpt4all<3,>=2.5.0 (from nomic[local])\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines->nomic[local]) (25.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->nomic[local]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nomic[local]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nomic[local]) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->nomic[local]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->nomic[local]) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->nomic[local]) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->nomic[local]) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->nomic[local]) (3.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->nomic[local]) (0.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->nomic[local]) (1.17.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.24.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.25.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, gpt4all\n",
            "Successfully installed gpt4all-2.8.2 jedi-0.19.2\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.13-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting diskcache\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-6.0.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.13-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-6.0.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=8e3569f28b30da317033c1433d4bc5e0f0a2d76af367c52563759f8ccbc202da\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, diskcache, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.13 coloredlogs-15.0.1 diskcache-5.6.3 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 overrides-7.7.0 posthog-6.0.0 pybase64-1.4.1 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        " OPENAI_API_KEY = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model"
      ],
      "metadata": {
        "id": "DwOkiv5yh_l6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remplacer le MultiQuery par la méthode Step-Back"
      ],
      "metadata": {
        "id": "jseLXvVppZKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L’approche Step-Back Prompting est une technique avancée d’ingénierie de prompts visant à améliorer les capacités de raisonnement des modèles de langage (LLMs). Elle encourage le modèle à prendre du recul, à généraliser une question avant de chercher une réponse détaillée, ce qui renforce la cohérence et la pertinence des réponses.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏗 Comment fonctionne Step-Back Prompting ?\n",
        "L’approche repose sur deux étapes clés :\n",
        "\n",
        "* 1️⃣ Reformulation en question plus générique\n",
        "\n",
        "Avant de répondre directement à une question spécifique, le modèle reformule cette dernière sous une forme plus générale.\n",
        "Cela lui permet d’identifier les principes fondamentaux et d’éviter des biais liés à des détails superflus.\n",
        "\n",
        "* 2️⃣ Utilisation de la reformulation pour guider la réponse finale\n",
        "\n",
        "Le modèle répond ensuite en tenant compte de cette abstraction, ce qui améliore la clarté et la précision du raisonnement.\n",
        "\n",
        "---\n",
        "## 🔍 Exemples concrets\n",
        "* 🎯 Exemple 1 : Compréhension en histoire\n",
        "\n",
        "📌 Question originale :\n",
        "\n",
        "🧐 \"Quel a été l'impact économique du blocus de Napoléon sur l’Europe ?\"\n",
        "\n",
        "📌 Reformulation Step-Back :\n",
        "\n",
        "🔄 \"Quels ont été les principaux effets économiques des blocus militaires dans l’histoire ?\"\n",
        "\n",
        "📌 Réponse finale basée sur Step-Back :\n",
        "\n",
        "✅ En expliquant d’abord comment les blocus économiques influencent généralement les nations, le modèle est mieux préparé pour analyser le cas spécifique du blocus napoléonien.\n",
        "\n",
        "---\n",
        "\n",
        "* 🎯 Exemple 2 : Sciences et physique\n",
        "📌 Question originale :\n",
        "\n",
        "🧐 \"Que se passe-t-il lorsque l'on double la température d’un gaz parfait tout en diminuant son volume de moitié ?\"\n",
        "\n",
        "📌 Reformulation Step-Back :\n",
        "\n",
        "🔄 \"Quels sont les paramètres influençant la pression d’un gaz parfait ?\"\n",
        "\n",
        "📌 Réponse finale basée sur Step-Back :\n",
        "\n",
        "✅ Le modèle identifie d’abord la loi des gaz parfaits (PV = nRT), puis applique cette connaissance au cas spécifique.\n",
        "---\n",
        "\n",
        "## 📖 Pourquoi utiliser Step-Back Prompting ?\n",
        "* ✅ Amélioration du raisonnement : Le modèle adopte une approche analytique avant de répondre, réduisant ainsi les erreurs de logique.\n",
        "\n",
        "* ✅ Généralisation efficace : Il est capable de traiter des variantes similaires d’un même problème en identifiant les principes sous-jacents.\n",
        "\n",
        "* ✅ Moins de biais et d’hallucinations : En évitant de se concentrer trop tôt sur des détails, le modèle limite la propagation d’erreurs.\n",
        "\n",
        "---\n",
        "## 🏗 Mise en œuvre avec LangChain\n",
        "L’implémentation peut être réalisée avec un prompt structuré incluant des exemples Few-Shot pour guider le modèle.\n",
        "```python\n",
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "# Exemples d’application de Step-Back\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Quel est l'impact de la révolution industrielle sur la population urbaine ?\",\n",
        "        \"output\": \"Quels sont les effets généraux des révolutions technologiques sur la démographie urbaine ?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Comment fonctionne un moteur à combustion interne ?\",\n",
        "        \"output\": \"Quels sont les principes généraux des moteurs thermiques ?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Création du modèle de prompt avec exemples\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"Tu es un expert en connaissances générales. Ta tâche est de reformuler une question en une version plus générique et plus simple à traiter. Voici quelques exemples :\"\"\",\n",
        "        ),\n",
        "        few_shot_prompt,\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Génération des reformulations Step-Back\n",
        "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
        "question = \"Quels sont les effets économiques des cryptomonnaies sur les banques traditionnelles ?\"\n",
        "generate_queries_step_back.invoke({\"question\": question})\n",
        "```"
      ],
      "metadata": {
        "id": "DQwHII8vpjGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Réalisation"
      ],
      "metadata": {
        "id": "P6UW4ZcYsxdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.documents import Document\n",
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph\n",
        "from operator import itemgetter\n",
        "from langchain.load import dumps, loads\n",
        "from functools import lru_cache\n",
        "from diskcache import Cache\n",
        "from langsmith import traceable, Client\n",
        "import json\n",
        "\n",
        "multiquery_cache = Cache(\"./multiquery_cache\")\n",
        "response_cache = Cache(\"./response_cache\")"
      ],
      "metadata": {
        "id": "ZeSHJLslszEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c99ac04-72d0-4970-e185-c5abd4f35d07"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScoredQuery(BaseModel):\n",
        "    query: str\n",
        "    score: float = Field(\n",
        "        ge=0.0, le=1.0,\n",
        "        description=\"A relevance score between 0 (bad) and 1 (very relevant)\"\n",
        "    )\n",
        "\n",
        "class StepBackOutput(BaseModel):\n",
        "    general_question: str = Field(description=\"Version plus générale de la question initiale.\")\n",
        "    reasoning: str = Field(description=\"Explication de la généralisation de la question.\")\n",
        "\n",
        "class ResponseFormatter(BaseModel):\n",
        "    answer: str = Field(description=\"The response to the user's question.\")\n",
        "    sources: List[str] = Field(description=\"The sources used to generate the answer.\")\n",
        "\n",
        "\n",
        "# Enhanced state definition\n",
        "class RAGState(BaseModel):\n",
        "    query: str\n",
        "    sources: List[Dict[str, str]]\n",
        "    available_docs: int = 0\n",
        "    ingested: Optional[List[Document]] = None\n",
        "    retrieved_docs: Optional[List[str]] = None\n",
        "    multiple_queries: Optional[StepBackOutput] = None\n",
        "    answer: Optional[ResponseFormatter] = None\n",
        "    feedback_input: Optional[str] = None\n",
        "    should_continue: bool = False\n",
        "    cache_hit: Optional[bool] = None"
      ],
      "metadata": {
        "id": "SGCM2qaRsz1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "def get_chat_model(config):\n",
        "    model_type = \"openai\"\n",
        "    model_name = \"gpt-4o-mini\"\n",
        "    model_temperature = 0.3\n",
        "    model_max_retries = 3\n",
        "\n",
        "    print(f\"Selected Configuration:\\n\"\n",
        "          f\"Model Type: {model_type}\\n\"\n",
        "          f\"Model Name: {model_name}\\n\"\n",
        "          f\"Temperature: {model_temperature}\\n\"\n",
        "          f\"Max Retries: {model_max_retries}\")\n",
        "\n",
        "    if model_type.lower() == \"ollama\":\n",
        "        # Return ChatOllama if the model type is \"ollama\"\n",
        "        return ChatOllama(\n",
        "            model=model_name,\n",
        "            temperature=model_temperature,\n",
        "            max_retries=model_max_retries\n",
        "        )\n",
        "    elif model_type.lower() == \"openai\":\n",
        "        # Return ChatOpenAI if the model type is \"openai\"\n",
        "        return ChatOpenAI(\n",
        "            model=model_name,\n",
        "            temperature=model_temperature,\n",
        "            max_retries=model_max_retries,\n",
        "            api_key=OPENAI_API_KEY\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unrecognized model type: {model_type}\")\n",
        "\n",
        "\n",
        "def get_embeddings(config):\n",
        "    model_type = \"openai\"\n",
        "    model_name = \"text-embedding-3-small\"\n",
        "\n",
        "    print(f\"Selected Embedding Configuration:\\n\"\n",
        "          f\"Embedding Type: {model_type}\\n\"\n",
        "          f\"Embedding Model: {model_name}\")\n",
        "\n",
        "    if model_type.lower() == \"ollama\":\n",
        "        return OllamaEmbeddings(\n",
        "            model=model_name\n",
        "        )\n",
        "    elif model_type.lower() == \"openai\":\n",
        "        return OpenAIEmbeddings(\n",
        "            model=model_name,\n",
        "            api_key=OPENAI_API_KEY\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unrecognized model type: {model_type}\")\n",
        "\n",
        "\n",
        "#### 📥 Agent d'Ingestion ####\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "import requests\n",
        "\n",
        "def ingestion_agent(state: RAGState) -> RAGState:\n",
        "    all_docs = []\n",
        "\n",
        "    for source in state.sources:\n",
        "        src_type = source.get(\"type\")\n",
        "        path = source.get(\"path\")\n",
        "\n",
        "        if src_type == \"web\":\n",
        "            loader = WebBaseLoader(web_paths=[path])\n",
        "        elif src_type == \"pdf\":\n",
        "            loader = PyPDFLoader(path)\n",
        "        elif src_type == \"file\":\n",
        "            loader = TextLoader(path)\n",
        "        elif src_type == \"api\":\n",
        "            response = requests.get(path)\n",
        "            if response.status_code == 200:\n",
        "                content = response.text\n",
        "                all_docs.append(Document(page_content=content))\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"⚠️ API inaccessible : {path}\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"⚠️ Type de source inconnu : {src_type}\")\n",
        "            continue\n",
        "\n",
        "        docs = loader.load()\n",
        "        all_docs.extend(docs)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    splits = text_splitter.split_documents(all_docs)\n",
        "\n",
        "    return {\"available_docs\": len(all_docs), \"ingested\": splits}"
      ],
      "metadata": {
        "id": "xXqGlE07yIpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cached_stepback(question: str) -> StepBackOutput:\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\n",
        "         \"Tu es un expert en raisonnement logique, en formulation de requêtes et en recherche d'information.\\n\"\n",
        "         \"Ta tâche est de transformer une question spécifique en une version plus générale ou abstraite, \"\n",
        "         \"afin d’élargir le champ de recherche et d’augmenter les chances de trouver des informations pertinentes.\\n\\n\"\n",
        "         \"Tu dois également expliquer brièvement la logique derrière ta reformulation, en mettant en évidence \"\n",
        "         \"les éléments que tu as généralisés et pourquoi.\\n\\n\"\n",
        "         \"Ta réponse doit être structurée avec deux champs :\\n\"\n",
        "         \"- general_question : la version généralisée de la question initiale\\n\"\n",
        "         \"- reasoning : une explication claire et concise de ta démarche de généralisation.\"\n",
        "        ),\n",
        "\n",
        "        (\"human\", \"Comment ajouter un outil à un agent LangChain ?\"),\n",
        "        (\"ai\", '''{{\"general_question\": \"Comment utiliser des outils personnalisés avec LangChain ?\",\n",
        "\"reasoning\": \"La question initiale porte sur l'ajout d'un outil, mais c'est un cas particulier de l'utilisation générale d'outils avec LangChain.\"}}'''),\n",
        "\n",
        "        (\"human\", \"Comment récupérer des documents d'une base vectorielle avec LangChain ?\"),\n",
        "        (\"ai\", '''{{\"general_question\": \"Comment fonctionne le mécanisme de récupération (retrieval) dans LangChain ?\",\n",
        "\"reasoning\": \"La récupération depuis une base vectorielle est un cas spécifique de la logique de retrieval dans LangChain.\"}}'''),\n",
        "\n",
        "        (\"human\", \"{question}\")\n",
        "      ])\n",
        "\n",
        "\n",
        "\n",
        "    target_llm = get_chat_model({\n",
        "        \"configurable\": {\n",
        "            \"model_type\": \"openai\",\n",
        "            \"model\": \"gpt-4o-mini\",\n",
        "            \"temperature\": 0,\n",
        "            \"max_retry\": 2\n",
        "        }\n",
        "    })\n",
        "\n",
        "    structured_llm = target_llm.with_structured_output(StepBackOutput, method=\"function_calling\")\n",
        "    chain = prompt | structured_llm\n",
        "\n",
        "    return chain.invoke({\"question\": question})\n",
        "\n"
      ],
      "metadata": {
        "id": "ImBVBlu-xdiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_cache_agent(state: RAGState) -> RAGState:\n",
        "    cached = multiquery_cache.get(state.query)\n",
        "    if cached:\n",
        "        return {\n",
        "            \"multiple_queries\": cached,\n",
        "            \"cache_hit\": True\n",
        "        }\n",
        "    return {\"cache_hit\": False}\n",
        "\n",
        "def return_cached_answer(state: RAGState) -> RAGState:\n",
        "    answer = response_cache.get(state.query)\n",
        "    if not answer:\n",
        "        raise ValueError(\"❌ Cache inconsistant : réponse absente malgré cache_hit=True\")\n",
        "\n",
        "    print(f\"✅ Returning cached answer for query: {state.query}\")\n",
        "    return {\n",
        "        \"answer\": answer\n",
        "    }"
      ],
      "metadata": {
        "id": "1JaLPeYC59Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiquery generation agent with structured output\n",
        "def stepback_agent(state: RAGState) -> RAGState:\n",
        "    result = cached_stepback(state.query)\n",
        "    multiquery_cache[state.query] = result\n",
        "    return {\"multiple_queries\": result}"
      ],
      "metadata": {
        "id": "4FsMVWNMs5p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unique_union(documents: list[list]) -> List[Document]:\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "\n",
        "# Modified retrieval agent to use structured multiquery output\n",
        "def retrieval_agent(state: RAGState) -> RAGState:\n",
        "    if not state.available_docs > 0:\n",
        "        raise ValueError(\"No documents indexed!\")\n",
        "\n",
        "    target_embedding = get_embeddings({\n",
        "        \"configurable\": {\n",
        "            \"embedding_type\": \"openai\",\n",
        "            \"embedding_model\": \"text-embedding-3-small\"\n",
        "        }\n",
        "    })\n",
        "\n",
        "    vectorstore = Chroma.from_documents(documents=state.ingested, embedding=target_embedding)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    # Retrieve documents for each query\n",
        "    all_docs = []\n",
        "    docs = retriever.invoke(state.multiple_queries.general_question)\n",
        "    all_docs.append(docs)\n",
        "\n",
        "    # Get unique documents\n",
        "    unique_docs = get_unique_union(all_docs)\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in unique_docs)\n",
        "\n",
        "    return {\"retrieved_docs\": [formatted_docs]}"
      ],
      "metadata": {
        "id": "oGlXJFM7s9Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restitution agent remains the same\n",
        "def restitution_agent(state: RAGState) -> RAGState:\n",
        "    if not state.retrieved_docs:\n",
        "        raise ValueError(\"No documents retrieved!\")\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "    Answer the question using only the following context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Return your response in a structured format with the answer and the sources used.\n",
        "    \"\"\")\n",
        "\n",
        "    target_llm = get_chat_model({\n",
        "        \"configurable\": {\n",
        "            \"model_type\": \"openai\",\n",
        "            \"model\": \"gpt-4o-mini\",\n",
        "            \"temperature\": 0,\n",
        "            \"max_retry\": 2\n",
        "        }\n",
        "    })\n",
        "\n",
        "    structured_llm = target_llm.with_structured_output(ResponseFormatter, method=\"function_calling\")\n",
        "    chain = prompt_template | structured_llm\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"context\": state.retrieved_docs[0],\n",
        "        \"question\": state.multiple_queries.general_question\n",
        "    })\n",
        "\n",
        "    response_cache[state.multiple_queries.general_question] = response\n",
        "    return {\"answer\": response}\n"
      ],
      "metadata": {
        "id": "dm7IxLAMtCPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client()\n",
        "\n",
        "#### 💚 Agent de Feedback ####\n",
        "\n",
        "def feedback_agent(state: RAGState) -> RAGState:\n",
        "    feedback = state.feedback_input.lower()\n",
        "\n",
        "    # Si feedback utile, modifier la requête\n",
        "    if any(word in feedback for word in [\"pas clair\", \"exemple\", \"vague\", \"mauvais\", \"recommence\", \"précis\"]):\n",
        "        print(state.query)\n",
        "        new_query = f\"{state.query} (Améliore avec des exemples et plus de clarté, l'utilisateur a dit: '{feedback}')\"\n",
        "        return {\n",
        "            \"query\": new_query,\n",
        "            \"should_continue\": True,\n",
        "            \"feedback_input\": None  # reset\n",
        "        }\n",
        "\n",
        "    # Feedback pas utile ou non pertinent\n",
        "    return {\n",
        "        \"should_continue\": False,\n",
        "        \"feedback_input\": None\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ask_feedback_agent(state: RAGState) -> RAGState:\n",
        "    print(\"💬 Réponse :\", state.answer)\n",
        "    user_feedback = input(\"📝 Entrez un feedback (ou laissez vide pour continuer) : \").strip()\n",
        "\n",
        "    return {\n",
        "        \"feedback_input\": user_feedback,\n",
        "        \"should_continue\": bool(user_feedback)\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnGlYI-impVJ",
        "outputId": "cb40cae6-bf68-4087-e21c-8cb0585e3a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the graph\n",
        "graph = StateGraph(RAGState)\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"ingestion\", ingestion_agent)\n",
        "graph.add_node(\"check_cache\", check_cache_agent)\n",
        "graph.add_node(\"stepback\", stepback_agent)\n",
        "graph.add_node(\"retrieval\", retrieval_agent)\n",
        "graph.add_node(\"restitution\", restitution_agent)\n",
        "graph.add_node(\"return_cached_answer\", return_cached_answer)\n",
        "graph.add_node(\"ask_feedback\", ask_feedback_agent)\n",
        "graph.add_node(\"feedback\", feedback_agent)\n",
        "\n",
        "# Connect the steps\n",
        "graph.set_entry_point(\"ingestion\")\n",
        "graph.add_edge(\"ingestion\", \"check_cache\")\n",
        "# If the request is stored in cache, we given pregenerated response, else, we give the request to multiquery\n",
        "graph.add_conditional_edges(\"check_cache\", lambda state:\n",
        "    \"return_cached_answer\" if state.cache_hit else \"stepback\"\n",
        ")\n",
        "graph.add_edge(\"stepback\", \"retrieval\")\n",
        "graph.add_edge(\"retrieval\", \"restitution\")\n",
        "graph.add_edge(\"restitution\", \"ask_feedback\")\n",
        "graph.add_edge(\"return_cached_answer\", \"ask_feedback\")\n",
        "graph.add_edge(\"ask_feedback\", \"feedback\")\n",
        "\n",
        "# Conditionnelle : soit on relance, soit on termine\n",
        "graph.add_conditional_edges(\"feedback\", lambda state: \"retrieval\" if state.should_continue else \"end\")\n",
        "\n",
        "\n",
        "# Compile the pipeline\n",
        "executor = graph.compile()"
      ],
      "metadata": {
        "id": "bF9DV-rbtEZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(executor.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "9HfPRtBitG_y",
        "outputId": "855d28fb-e79a-44c6-e9f4-2e6cbdfec2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGMAAAFrCAIAAAD6pNOZAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3Wd8U+Xfx/ErTdq06S50l7IECrRQoAiisspWBAQFARmCgCJDtjgQFGXLUESUIUtkCiobQQRFWaWlLKFAyyildI80zbgfxH/uemzDCk3Sft6vPkjOybnyO/t8c0ZlBoNBAAAAAACKcLB2AQAAAABgc0hKAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABAiqQEAAAAAFIKSzWUnqK5c70gN0OnKdBbqk075eIm9w1RBlZ1tnYh9+X2NfWdGwV5Wbpy+7h4F3d5xSBlUDV7mV8FqTcLcrO1Bp21S3n8VO7yisF2syolX1HfuVGQl10OZsz9UaocvHydKtV0kStk1q7l3jLuaO9cV2dnFBaqy+umEPfHxV1eIdApuLqLtQu5LymJBXduFuRlafVsmR4nB7nM1VNeMVDpF6q0di335cal/Lu3NPk55XexkDvJXN3lFYKc/UKczH9SZpH/p3R0x927yYVCCN9KLpr88jvdjQo1+rs3C3Rafdc3gpUutnvWzmAQP31zU6cTKneFm5ejXldOjw+0WsOdpHyd1vDCkCCVu9za5Zjz8/JbOq1wdpW7ezvptGX/Jwl1ri4rrVAYDC8MDVI42u7RtrbQsP2rm0Im8/BxdHa16UWoNCmc5MlXcwvy9E07+lSurbJ2Oeb8uSst9aZGCOFXyaWg3O/CYJ5OZ7iTqNYW6jsPCXL1sOn1fee3yYUFBqVK7uHjqNOW01186VA4OmSmajQFeielrP2r/tYux5zsdO3Py24pnBz8K7vIbHe/+tg5OTukJRfotAYXN4fWL/uZ+aQFktKfu9NzMnVPtq/4iO2UMak3Ck7uT31+UKBSZaNhafOiG3WaeofUtOkjmFKTflvz1+47nQYE2GxY2vrFzRoNPSvXcbV2IaUt+Wp+7K9pXd8IkttkWNIVGrZ+ebN+C5+AKvbxG3Pp27P6ZtOOPsHVbfTc4LG96Zmp2iadfK1dCOxJZmrh0Z9TOvYPcPW00V3GtqU3q9R2r1bP3dqFlC+XYrKuX8zt/HqgtQspXlaads+a2806+7n7OFq7FlsR/0dGdpqmbe8Sw9KjHsTH/5GVnlJITPqvisHKqLYVty6+Ye1Civfz8lthjb2ISSbe/k5NO/ltXnTd2oUUb/fq29XquZfDmCSECKjiUq+5z7alN61dSPG2Lb1ZvzkxyZx2rwbtWZ2ck6G1diHFOPdXVupNDTEJD8qzouPTL/hvWphk7UKKt++7lNCabsSk0vdEpEdgNdUvG1KsXUjxNi1MerabPzGpqLpPeancFb//dLekDzxaUjKIuMOZ9Zv7PFIjZZdPoNLNy/HauTxrFyKVfluTk6ELrV0eD7vN8KzoWDHIOSE219qFSGWn6VJvFpTnfV5AVRchZMlX1NYuROrWFbUQsoCqxKR7qN/C5+SBDGtXUYzTh7PYheHhuHkrAqqqLp7KsXYhUrmZultX1U808LB2IeVUrSjPpAt56lybu0L+wvHsSjXcVB4We0JBmVGvuU/8H5n6EubYIyUlTYE+O6PQzYuJXiIPH8c7NwqsXYVU6k2Nuze/KBTDo4KTTc4vtUe5/wXIw9sx9ZbG2lVIpd3SMGvuh5ef0+1Emwu6Oq0hI6WAn1fx0Dy8HVNv2twu4+6tAndvDsysyd3byQYXjDs3CjwrsrkrnpuXY1oJxxiPlJRys3Tcvmye0lVug48Wyc3WKplxxXFxk+dm2dw1QrlZWmdVed/tKV3ledm2N2uytUoVq9K9ubgp8mxvzcrL1jkz+/AInN3keZm2uItnl2FdLm7y3Eyb2+LlZOo49iuJc8mHfzb6sAEAAAAAsCKSEgAAAABIkZQAAAAAQIqkBAAAAABSJCUAAAAAkCIpAQAAAIAUSQkAAAAApEhKAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABAiqQEAAAAAFIkJVjZlA8njB33hqVa69ItetXqbyzVGh7T9Ny8ZX2bdk0s3mw5lJBwqVV0VGzsqYdugVUGEi/17PjNsi8esZGBg16ev2CGhSqCLXqITUdCwqWJk0a0bd907boVFqnhwMG9raKjMjLSLbspe/Ttaplx+MjB14f0bhUdFR8fa5EG5y+YMXDQy8bX9jLL7DspXblyuVfv5x9iwKnTJu3Yue0xVFTubP1hw6czp1i7Cjw88ytRz5dfrRfRoHQrwj0UnWVeXt79Xh3s5xfwQNvDbt3b3rx1w/iaWfz4FJ3OAPb/sis27tTUKbOiW3ewdi24L9+t/9YgDPPmLqlcuZq1a7EahbULeCQXLp59yAEvnG3c+ClLl1MeXbjwkLMANsL8StT7lQGlWAvuS9FZ5uNTYeCAYf/tbkZy8i3jT7BGzOLHRDKdAeTm5gQEBDVr1tzaheB+5eXl1q/XsEFklLULsSb7SErZOdkrVi758+jh9Iy0WjXrtGnT8blOXVesXGI8bdcqOurNN95+qUefLVu/P3r0t3PnzjgplfXrNRw0aHhwUIjxUp913614e/Q7Uz6c0LXry1u2rBdCzJ7z0ZdLPvtx20Frj5x9SEy8umLlkpjTJwwGQ9269Xq93C8iInL0mCGnT58UQuzZ8/NXS9bUrBEWHx/77aql58/He3p5P9X02f79hri6ugohNmxcs+67lePGvDdv/icZGelBQSH9+g5u1+45Y+Mymez4iT+//37VmfjT1avXHDliQs0aYUKInJycjZvW/HXsj6tXL1fwqdisWYvXBr7h7OwshNDpdBs3rf121VIhRJ3aEQP6D42IiJTUHBNzYvzE4cPfHNu1y0vWmGbW1KVbdL++gw8d/iU29tS2H37xcPcodtZIVqJGDZsMer3Xp9Pnz5n3sZeX9zdLv+vSLbr7i6/0e3WwECIt7e7iL+ediT+tVqsbN36qX9/BlSpVPnb86ISJby1asCw8vL7xq8+dj39zeP9PP1nQtMnTJa2S5dl/Z82u3T9u/3HzlSuXqlZ9onWrdt1ffEUmk93nds84yxZ89vWJk38W7R4eHvnm8P6Lv/i2dlhd4/f2fbVrs2Ytnnrq2TFjhwkh+vTt8vTTLT6eNrfoLE5MvDp/wYyLf5+TyxVVqlQb0H+ocQe59YcNq9d8M3/e0ilTJ1y9mlCt2hMv9ejToX1na09LK5DsTUYMH1fsenEq5rhkOnd87pn+/Yb06tnP2M6s2dMuX7741ZI1/10kPvvsE5lM1ia644xZH+bn59WpEzFsyKjatcPNF1bSJvHKlcvbf9x08tSx5OSbVSpX69Spa5cXepgfRAihUDhu2fr9kq/mOzk5hYdHvjNpmqeHZ0kbAeMgV68mzJg55VrilcjIqH59B9/n9CxpE2FmkSt2vfjo48np6Wnz5i4xNtt/YI+MjPRtW/cb33708eTcvNwZnyzQarXLli8++ufhlJTk8PDIbl1ebtr0GeOlO5JN34MvGvbNzN622L2/ZPD72duOGDXozJnTxm3U4EHD+/QeWNKmTwhhpteSrxbs2fuzykUVHd0hJKSy5Fu2/rBh167tN24mNWzw5Ji3J3t5eZtfC7Kys776asGOnds8Pb2iGjV5ffAIf/8ASZurVn+z7rsVn81batqclgcGg6F1m8bGVXvb9k2fL1xet269h5hleXl50z9979SpY1WrPtGlc4//ftHjm2WLP/+2WrUnHn1S2MfVd7NmTT0bHzt69Dsrl2+qXTv8s/mfxsfHDhwwrFfPfv7+AQf2H3+pR5+4uJhFn8+uW7f+tGlzJk2cmp6eNv2T94yDOzk55eXlbt++6Z1J07p1eXnXjiNCiPHj3icm3SeNRjN6zBC5XD5zxqK5s79UyBXvvve2Wq2eP29p7drh7do9d2D/8Zo1wq7fSBo34U11gfrzRSs+mjonIeHvt8cM0Wq1Qgi5XJGbm7P/l11rV2/7Yev+6NbtZ8z6MCnpmrH9a4lXfti2oXfvgZ9Mn6/X6997f4zBYBBCbNm6ft13K3u+/Oon0+cPHTrq4K97jTt1IcTSrxdt27Zx2tQ5702e7uvrP/GdEYmJV4vWfO3alfc+GPPCCz3KYUwSQjg6Ov60Y+sTT9SaPesLlYuqpFkjWYkcHR2FEKvWfNPz5VfHjnmvaIM6ne7tsUNjTp94e/Tk5d987+3l8+bw/jduXm/YoLG7m/uh334xffLw4QPubu6No5qaWSXLM8ms2bd/18xZU2vWCFu3ZvvgQcM3bV73+eK5xk/ez3bP1GxJ3SUaREZ9On2+EGLtmm0fT5tbtFd6etpbIwb6+QUs/WrdF4tWeHv5fPTx5Ly8PGPNOTnZCxfNGj/2/V/2HWvRvM2s2dNu305+nNPJRkn2JiWtF2am839JFgmFQhF/Nnbvvh1Lvly98+fDSifl/VzhXNIm8YvFc48d+2PUyIkzPl3YqVPXBQtnHv3ziPlBhBC/HtqXm5szc8ai8eM+OHMmZsWKL81sBIQQhYWFE98Z4evrv3L5pqGvj1z//aq7d1PvWbOZTYSZRa7Y9aJhwyfPnT+j0+mMS/Lt27eEENevJ/7zRWdioho1EUIsXDRr0+Z13br2XLf2xxbNo6dMnfDrof3Grytp01dOlLS3LWnvX3TY+9zbLlqwrMsLPapUqXZg//E+vQea2fSZ6bVt+6Zt2zeOGjlx8eJVgYHBq1Z/XfQrdu7clp5+d9iw0e++83FMzPHPv5hj7F7SWqDVaie9MzL17p15c5eMeGt8yp3bkyaPNB60mOzbv2vFyiXvv/tJuYpJxp+wD+w/XqVKtS4v9Diw/3jduvUebpbNmfvR9euJc2Z/+dHUOVeuXj765+Gi3/JYZ5lFYpLdnFM6HXuyV89+jaOaCiGGvD6iRYs2nh5eks/UqROxYtmGkJBQhUIhhNAWFk5+7+3MrExPD0+ZTKZWq3v16t+wQWMhREFBgZXGw14lJV1LT0/r/uIrxlM9Uz6YcTr2pGTRFELs27fTUeH40dQ5np5eQohxY99/pU/nw0cOtmzRxrh8v9itl4uLi4twGdB/6JYt6/f/sntA/yHGHdvokZMqVvQVQvR79fV3Jo86ffpkZGSjl1/q26J5dOXKVY3tnzlz+q9jvw8dMjIzK3PDxjWjR00yLhJNmjydl5d7Ny01NLSK8ZN376aOm/BmRESD4W+MKfWpZRNkMpmHh+eI4eOMb83PmqJDCSEaRzX976F2XFxMYuLVuXO+NK5EbwwbfeT3XzdvXjdyxIRWrdod+m3/m2+8bfzkod9+iY7uIJfLzaySpTUZbJFk1uzY8UO9eg1Gj5okhPD29hnYf9isOdP69n7N29vnfrZ7FrRx01onpXLc2PeM82v8uA96vNx+2/aNr/TqbzwU7t9vSJ06EUKI9u2eX7FyyaVLF/77S16ZJ9mbxMScKGm9eKA2iy4SQoj8vLzx4z5QqVRCiOjWHWbM+jAvL8/4tlhmNonvv/9pXl5uYECQMSfv2rX9r2O/N23ytPmtqErl+mrfQcbGj/z+a2zcKfMbgUO//ZKScnvBZ98YF4mRIya81LPjPUfc/CaipEWu2PWiYkU/tVqdcOVSjSdqxZw+Ua1aDTdXt9OxJ0NCQpOTb925k9KoYZOCgoLde37q/cqAFzp3F0J06tjlzJnTq1Z/3aJ5tJlNXzlR0t72nnv/h97bmtn0mem1Zev6Fs3btGgeLYTo0L7zuXNnTHlYCOGiUg0cMMw4N59//sVNm9dpNBonJ6eS1oKjfx4+d+7Mtys2GRf7SpUqb9i4Ji3trqnBmJgTM2d9OHTIyKefbmGJyWzfHmKW6XS6Awf3TpwwpU7tcCHE0CEjf//jUNE27WKW2UdSioiI3LBxTWZmRv16DRs3fqpWzdr//YxcLr958/oXi+eeO38mNzfX2DEjPc10WBZWq3z9HmBBISGhXl7eM2Z92LZNp8j6jcLD6xd70Wp8/OmwsLrGY3EhREBAYFBQSGzcKdPheM3/zTiZTBYUFJKYeMX4tnq1GsaYJIQIr1tfCHHz1vXIyEaOjo7Hjv8xY+aUS5cvGjfN3t4+QoirVy4LIcL+9wOPQqGYNnW2qeWCAvWESW95eHhOeX+Gg4N9nDV9HGrVrGN6fc9ZU1TNGsWsX3FnYhwdHY1HSMbpHFm/0enYk0KIli3bbv9x88W/z9esEXblyuXr1xMnjp9yP6tkuWWaNXq9/kz86X6vvm7q1aBBY71eHxt3qkXz6PvZ7llQwpVLNWqEGY9ZhRCurq6VQipfvHjO9AHTGufu7iGEyMnJfqz12DLT3sTMevFAiq6tQohKoVVMucjNzV0IkZ2dZSYpmdkkCoNhy5b1f/51xHQOPzAw+B6DCBER/v+XV3l6eGkKCsyP7I0bSc7OzgEBgcZeFSpU9PPzv+dY33uvXdwiV9J6ERQUEhcXU+OJWnFnYsLr1ndxcYmPj32uU9fY2JMVKlSsWrV6XFyMRqNpHPX/tyhH1m+0c9f2zKxM49tiN33lREl7WzN7/0fZ25rZ9D37TKuSejV/tvWNG0kdO7xg6lXz31vFqEZNTReD1akTUbi+MPXunaDA4JLWgsuX/1apVKbfWGvWCHtv8semJS0x6eqSr+ZHt+5gumi2PHu4WebjXUEIUfRpELVq1fn77/Omt3Yxy+wjKU2c8OH27Zt+ObB7w8Y1bq5u3br17Pfq66Y9utGRI7++98HYPr0HDh0yqnr1GsdP/Dlh4ltFP+Dk5FTqhZcRSqVywWdf/7zjh02b1y1bvjgoKGRAvyFt23aSfCwnJ/v8hbOtov8VotKLZH2lUvn/r52dc3NzjK9dXd1M3Y1HA1lZmcaLQ3bs+GHo0FGNo57y9w/4ZtkXxicWGlcJZ6Xzf0s1GAwbNq7RarV16kSU8zledPTvOWv+NWCR2VS0hcLCQkkLxuuJI+s38vb2OXRof80aYb8dPuDr62e8Z+meq2S5ZZo1Go2msLBw2fLFy5YvLvqB9PS0+9zuWVDa3dTg4EpFuzi7uOTl55nemvZnMM1BM+vFwzVo9KA/8ZS0SdTr9ZMmjyos1Lw++K3IyCh3N/cRowaZH8So6GJmmu9mRjYrK9PF5V9BTllCy0XdcxNR7CJX0nrRsEHj+PjTL3brefr0iYEDhimVzgsWzhRCxMadatCgsWmUTVPAJD3trnF8i930lRMl7W3N7P0fZW9rZtNnpldubq5Opyu6pDk7uxT9jErlanpt/FhmZkaAf2BJa0Fubo6ZBXXBwplardbHp8IDjVpZ9XCzTC6XCyFURWaZix3OMvtISh7uHn37vNan98AzZ07/dvjA6jXL3NzcX36pb9HP/LRja0RE5OBBw41vy/PvnY9DaGiVN4aNHjhg2MmTf+3ctf2TGR9UrlLNeDrexKdCxYiISNOTuIyKXi+Um5trfMCDEKJArfb28jG+zlfnmz6Tk5sjhPDw8DQYDD/+tLlH997PP9ftn17/m6fGZJWXl1tsqTVqhA0ZPGLS5JGrVn89oP9QC00A+3bPWXNPFSpUdHFxmf7xZ0U7yh3kxqOZVq3aHT5ycPCg4YcPH2jb5p8IzSp5T87OziqVql3b55o3jy7aPSgw5D63e/dPq5NeLiuhcnVVF/zr9oP8vLyQ4NCH+7pywsx6YZ5Or7NgGSVtEi/+ff78+fg5sxc3aviksUtOTrZvRb97bkWLZWZkPTw884uE6vts+eE2ESWtF40aNfnqqwWZmRkJCZcaNnjSeMIqMzMj7kxM714DhBAVKvoKIcaOeVfyi4CfX0Ba2r3vqirDzOxtze/9H3pva2bTZ6aXq6urXC4vKLKZkix16iLHEsafYj09vcysBSqVa35+nl6vL/a3ifbtng8Lqzt33vSoqKamU6nl1sPNspSUZCFE0T2LZMtgF7PMDpJSZlbm/v27OnXs4uzsHBERGREReenShYtFTt4ZZWVlBvgHmt7+VuQWczyixMSr8WdjO3Z4wdnZuVmz5k2aPN2h09MXL56TJKXq1Wrs2ftz/XoNTUvw1asJISH/f6R1KubYM0+3NN4qlph09amnnv1f+1fUarXxMTvGx46HBIcWFhbm5+dXrOhn/IxGozFd3vrEE7UUCsXp2JPG50EZDIZ33h3dqkXb9u2fF0I0bfJMZGSjYUNHL1w068nGzYyXuZdz95w1926hes38/Hw/vwDTw+tu3rrh5fnPb+etW7bbsmX90aOH/750YfI7Hxk7skrej+rVa2bnZJsuaCksLLx164afn/99bvdKonRSFj2MyMnJSU29Y36QWjXr7N7zU2FhofHu9qzsrGuJV0wPqESxzK8XRTk5KYse1ZkuKbGIkjaJXt4+Qgjf/21Fr15NuHo1oWqV6vfcij7oyAb4B6rV6oSES8ZbqC9dunjP5e3hNhFm1osGkVHJt2/t/2V39eo1jNcm1KpVZ9++nYmJV6Oimhp3K8brGkyrW3p6msFgUKlUaWn3MZXLLq1WW9Le1vze/1H2tiVt+sz0kslk/v6B8fGx4n9PjpA8HuDSpQum1xcunHVycvKt6Gdc14pdC8Jq1VGr1RcunjM+rSEx8eq8+Z+MGD7eeDKzXdvn6tVrcOzYH9M/eW/5sg1cN/4Qs8x4yHHmzGnjVbKFhYXHT/xZ9Ky7XcwyO7iLQyFXfLtq6YfTJp45czot7e6ePT//fem88SrqkJDQu3dTDx8+mJR07YnqNY8dP3oq5rhWq924aa1x2OTbt/7boFKp9PX1O/6/D5f6CNmfrKzMWbOnfblk/vUbSUlJ19auW6HVao03FAUHVzp37szJU8fS09N69Oij1+s/XzxXrVYnJV37aunC1wb3TLhyydiIg4PDli3rExOv6nS65Su+LCgoMP3vOWdnlzlzP8rKzsrISF+7brmfn39ERKSTk1NoaJWdu7bfuHk9MzNj1pxpEeGR2dlZubm5bm5ubdt02rZt485d20/FHF/0+ewTJ/6UPEW3a5eXmjR5eupHk0yXv5dnZmZN0ZXITAuNGj755JPN5sz56Pbt5MzMjB+2bRz2xqu7dm039q1bt56fn/+KlUuqVXuiSpV/rki+/1WyPHt90FtHjhzcsXObXq+Pi4uZ9tE7Y8YN02g097ndK9pU0e6VKlV2d3PfsXObwWDQarUzZk0x3ulhvAdGCHHw4N6z584UHbxz5+65uTlz502/fTv56tWET2d84Kx07tSxa+lODztjZr2QTOc6dSJ+PbQ/JydHCLF6zbLU1BQLllHSJrFK5WoKheL7DauzsrMSE68u+nx246imxnXwfrai9z+yzZq1cHJymjPvY7VanZp6Z9rH73jcxzHKQ2wizKwXnp5eNWuEbd68zrhvMt71umXr+mrVnqhQoaLx0u4B/YeuWv218YalXw/tHzfhzfkLZjzgxC6DHB0dS9rbmtn7mzzc3rakTZ/5Xq1atj302y8HDu41/lPUs2fjirZ55erlDRvX6HS6i3+f373np+bPtnZ0dDSzFkRFNQ0OrrR06cLfDh84dvzo/AUz7qTcNj3WwmjC+CkKhWLGfTx/ssx7iFlmvBp/5colSUnXCgoKPp7+ruSSWruYZXaQlFxdXad9ODs1NWXEqEHdX2q/fsOqYUNHd37+RePvGRHhke9PGbf/l92vvfZmkyebvff+mHYdnrp9O3nSxKlhtepMemfkvv27/ttmn96vnTx17P0Pxha97gslCQ+vP+btyfv273y1X7d+A7rHxZ2aN3eJ8YC483MvymSy8ROGX07428PdY9k337s4uwx9o2+/Ad1jTp8YP+5903knmUz28kt9x4wb1qZdkx9/2jxpwofGf8RRqC0Mr1s/NLTqSy93eKlnR51O9/FH84zr0vvvfuKsdB4wsEfffl0bNXxy8OC3nJXO3bq3uZV8c9TIiZGRUXPnTR8zdlhcXMy0D2eb7vAzmTRxqlarnTV7qjWmmW0xM2uKrkTmG/l0+vwWLdpM+/idri+22bJ1fZs2HV98sZepb8sWbS/+fb51q/amLg+0SpZbERGRS5esjY091a1723ET3szNzfn4o3lKpfI+t3tFmyra3dHR8f33Pz1/Pr51m8av9OncskXbwMBg48P3g4NCOrTvvGLlkq+/XlR08JDgSlM+mHHlyqVevZ8fPWaIEGLB/G9Ml8uiJCWtF5Lp/NbwcT7eFTp3adm2fdOCArXpdyJLKXaT6O8f8O7kj8+ei+vStfXk994ePGj4Cy/0OHfuTP+BPUoa5OFG1s3N7ZPp83Va7fMvtBjwWo8e3XtLjl2K9RCbCDPrhfE+8pu3bkRENDC+rVu33s1bNxpE/v9FOL169hs/7oN161d27tJywcKZQYEhY8eWx2eC/1dJe9sKFX1L2vsX9RB725I2feZ79e0z6LlOXRd9PrtVdNQfR397840xxjOiQgittvClHn3i42PbtGsyZuzQiPDIt4aPE0KYWQsUCsWcWYv1Bv0HU8ZPmPiWs4vLp58skNwL6urqOuX9GX/+eWTL1u8tNLHt1cPNsncmTatdO3zIsD7PdW7u7u7RqWMX4/wqhVm2d+8Oi4y4zFTxQ0hPKfzpm5tdh0v/8xdMzh7N0ORrn+1a0dqF/EvMoYy7ydon25deVZu3rF/85bz9e/8qtW98OH+fzMpIUbfu6WftQv7lzO+Zt65omj7va+1CrCnmYJrSWTzZ3sfahfzLX7vTCtQisqVtVWWDcjK0e1Zd7//+PQ7ES1l2unbzouvdR9lWVbAjl2Ky7t5Qt+ltW7uMc8eyrp1TP93FtqoqVw5vvV0tXFUryt3ahfzLrlW3A6upqkXYVlU2Yt+6mw1belWuXcwjRu3gnBIAAAAAlLJSfaLDnLkf//rrvmJ7aXVahbz4YiZO/ND4GIDHofMLJbZspqRl33x/P/8sAgAAizCzt3qse8lHse67ld99t7LYXpWrVPt84fJSrwiPUVxczOR3R5fUd83qH0z/0w+wI6WalAYPGt6nz2vF9iooKFCW8K8MTM+SfhyWLl1XUi8zJfF8/QfV/cXxZwkZAAAgAElEQVRe3Yvc0wIAeCBm9laPdS/5KDp37t6qVbtie5X0QyTsV0REpJmllJgEO1WqmyovL28v8cD/ku+xCgwIsnYJAADcgz3urdzd3N3duCmiHLHHpRQwj/uUAAAAAECKpAQAAAAAUiQlAAAAAJAiKQEAAACAFEkJAAAAAKRISgAAAAAgRVICAAAAACmSEgAAAABIkZQAAAAAQOqRkpKTs4OjUm65Ysogg16o3GxuEjmr5MJgsHYVtkinNag8FNauQspZJTeIcj+/DMLF9lYlFze5gVXpPmg1Bk9fJ2tXIeWkdHBysbmFCnZEpxMqD5tbhJQuchk/g1uXTDi72tyC4eYh1+vYYRVPLpcpVcXPskdamVw95HlZhXnZukdppGxLScr39re544OKgU63r+VbuwpblJKU7+PvaO0qpCoGK5Ovlvf5dTsx38f2ViUff6eURLW1q7ADd27ku3na3G8QSpWDJl+Xk6G1diGwV6nX8739bG+XEaS8lZBn7SrKtZuX8yoGK61dhZR3gNOdJHZYxbtxKc83qPhjjEf92SG8mVdCbPYjNlJWadT6zDuaqnVdrV2IVMVgpVIlT7+tsXYhtkWnNdxJUteIdLd2IVJevo5eFR3vXC+/G7jcTK1Wow+u7mLtQqSCq7toNQYOte/p2rmcuk08rF1FMeo+5XnlDLswPBSDuHEpr1aUzS3YHj6KCkFKfsSxllsJ+YFVXVxt72Rj7Sc9Ei/mWrsKW3TtbG6NBu5yR1mxfR81KT3Z3jstWX3xZNYjtlP26AoNBzfe6vRaoG2eBO80IODojhSO8EwMBvHLetudXx36Bxzbk5p1t9DahVhBQZ7u8NbbnV4LFMVvxKxKJjq9FnBkW3JBHqfWS3T4h5Qa9d0Cqzlbu5BiNG7rnZlacOFYprULgf3Zt+7mc4MCHWxyl9GxX8DxPXcy7vB7aGlLv605+Utqh37+1i6kGA4OomP/wH3rblq7ENty+1r+2aPpbV7xK+kDMotcZP/z8luuHo6OSgfvQKWusLxfBKnJ16ffLvj7VFbPMZV8AmzueiGTvGzdhs+SQsPc3LwUrp6Oen05nXGFGv3dGwV/n8rqMTLEr5LNnS43UefqNy5ICqnhpvKQu3mVi/mlztFlpWmuxGW/PCbUw8fmrt0yyUrTbpiXWDXCw8PH0dn2bqayFgcH2Z0kdW5mYVB15wYtvaxdjjk7Vya7uCkUTg4+gc46rd7a5cCmaTWGuzfVl09ndX0zxD/UdncZGrV+4/zrQU+oVG4KN28nvZ4F+zGSO8iy0gvVudqbl/N6jAxxcrbJAC2EEOJWgvqn5Ter1nWvGOwit7lLR0uPQiHLuKMpyNelXld3Gx4sV5T4W6xlkpIQ4sqZvNtJ+epcvXVvW7qScCUwMNDZxZo/Xrp5yH2ClOFP2dwZ+WKdP5aVcr0gP0ev05b9I+9iuXoqKgQ4hjfztHYh9+X88ew71wvyc3VaTdmfX64ect8QZR2bvGrrv87+mXXnekFuFieX/uHurXD1kIfWcq0YbLs/GJlcic9LSVTn5+nymIMwy9VT7uPvFN7MU2aDZ7n/48KJnJQkdUGeXlNAUnqMnJRyZ1eZX4iyZiObu4D/v/RaEX80M/1OYXm+sMjZVa50cfALUdZo4Gb+kxZLSjaiX79+kyZNqlOnjrULAQAAAGDHbPf8IAAAAABYC0kJAAAAAKRISgAAAAAgRVICAAAAACmSEgAAAABIkZQAAAAAQIqkBAAAAABSJCUAAAAAkCIpAQAAAIAUSQkAAAAApEhKAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABAiqQEAAAAAFIkJQAAAACQIikBAAAAgBRJCQAAAACkSEoAAAAAIEVSAgAAAAApkhIAAAAASJGUAAAAAECKpAQAAAAAUmUtKfn4+MhkMmtXAQAAAMC+lbWklJaWZjAYrF0FAAAAAPtW1pISAAAAADw6khIAAAAASJGUAAAAAECKpAQAAAAAUiQlAAAAAJAiKQEAAACAFEkJAAAAAKRISgAAAAAgRVICAAAAACmSEgAAAABIkZQAAAAAQIqkBAAAAABSJCUAAAAAkCIpAQAAAIAUSQkAAAAApGQGg8HaNVhA+/btHR0dZTJZamqqp6enQqGQyWQuLi4bNmywdmkAAAAA7I/C2gVYhqura2JiovF1amqqEEIul48cOdLadQEAAACwS2Xk6rs2bdrIZLKiXSpVqtSjRw/rVQQAAADAjpWRpNSjR49KlSqZ3jo4OHTt2lWpVFq1KAAAAAD2qowkJT8/v1atWplOK1WpUoUTSgAAAAAeWhlJSkKIXr16Va5c2XiHUufOnZ2dna1dEQAAAAB7VXaSkq+vb8uWLWUyWaVKlV5++WVrlwMAAADAjln42Xd52brUmxqNWmfZZu/TU/W6nah645lnnkk6XyhEYekXIJfLPCo4evs7OZSdBAoAAACURxb7f0r5ObpfNtxJvpofWtu1IFdvkTbtjou7PPlKvrOrvG5Tj7DG7tYuBwAAAMBDskxSysvWbfniRvNuAd4BTpaoyr4ZDOLQpuSqEa51mxCWAAAAALtkmavEVn9yrdNrIcQkI5lMtHgpICE25+LJbGvXAgAAAOBhWCApndyfEdnSx1HJrTn/8lRn/7gjmcIy1zYCAAAAKFUWiDe3ruW7eTlaopgyRenikHGnMC/HOg+3AAAAAPAoLJCUdBqDuw/X3RXDr5JL1l2ttasAAAAA8MAskJTycnR6fTl92J15+TlaweV3AAAAgB3i5iIAAAAAkCIpAQAAAIAUSQkAAAAApEhKAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABAiqQEAAAAAFIkJQAAAACQIikBAAAAgBRJCQAAAACkrJCUEhIutYqOio09VfpfbbR5y/rotk9a69sBAAAA2D4rJCUvL+9+rw728wsozS/d+sOGT2dOMb6uUzv81b6DS/PbAQAAANgXRel/pY9PhYEDhpXyl164cNb0unbt8Nq1w0u5AAAAAAB2xApJKSHh0qDXey347Ot69RpMnTZJJpO1ie44Y9aH+fl5depEDBsyyhhj9Hr9goUzDx856OToFB3dIbxu/XfeHb15424fnwparXbZ8sVH/zyckpIcHh7ZrcvLTZs+Y2w8MfHqipVLYk6fMBgMdevW6/Vyv4iIyNFjhpw+fVIIsWfPz18tWRMXF7P4y3n79/5lHGTV6m927/kpNTXFzy8gsn6jt0e/4+DgcOXK5dcG91z8xbfr1q04fOSgr69fq5bthrw+Qi6Xl/4UAwAAAFDKrPxEB4VCEX82du++HUu+XL3z58NKJ6XpGrmNm9b++NOWEW+NX7JkjYuLatnyxUIIBwcHIcTCRbM2bV7XrWvPdWt/bNE8esrUCb8e2i+E0Gg0o8cMkcvlM2csmjv7S4Vc8e57b6vV6vnzltauHd6u3XMH9h+vWSOsaAErVi75YduGN4aO3rRx96DX3jz4696Nm9YKIRwdHYUQc+d9HB3dYc+uP9595+MNG9ccOLjXStMJAAAAQKmy/rPv8vPyxo/7ICgwWKFQRLfukJR0LS8vTwixe89PzZ9t3bJFG08Pzz69B6pcXY2fLygo2L3np96vDHihc3dPD89OHbtEt+6wavXXQoikpGvp6WndX3ylZo2w6tVrTPlgxtSps7VabUlfnZ2T/d36b1/tO/iZZ1q6u7m3bNGmW9eea9YuKywsNH6gRfM2LVu0cXR0rF+/YVBg8MWL50prqgAAAACwJusnpUqhVVQqlfG1m5u7ECI7O0un0129mlC3bj3Tx5o/G218cfHiOY1G0zjqKVOvyPqNEhIuZWZlhoSEenl5z5j14Zq1y8+cOe3g4NAgMsrNza2kr05KulZYWFj0nqWaNWvn5OTcuJFkemvq5ebmnpOTbdFRBwAAAGCjrHCfkoTxgjqJnNwcg8GgUrmaunh6ev3TKydbCDFi1CDJIOlpd6tUqbbgs69/3vHDps3rli1fHBQUMqDfkLZtO5X01WlpqUIIZ6WzqYuLi0oIkZ+f5+7uUVJtAAAAAMo86yelYqlcVEII01VwQoj09LvGFxUq+gohxo55Nzi4UtFBjI8dDw2t8saw0QMHDDt58q+du7Z/MuODylWqSe5NMnF1dRNC5KvzTV3y8nKFED4+FQsLNY9t5AAAAADYOhs9Z+Lo6Ojn53/16mVTlyO//2p8ERIcqlQqhRANIqOMf1UqV6scWlWlUiUmXt25a7sQwtnZuVmz5h9OmalQKMzcXFS9ek25XB4ff9rU5dy5M+5u7r6+fo95/AAAAADYNBtNSkKIZk8137P352PHjxoMho2b1mZnZxm7q1SqAf2Hrlr9dVxcjEaj+fXQ/nET3py/YIYQIisrc9bsaV8umX/9RlJS0rW161ZotdrwuvWFEMHBlc6dO3Py1LH09DTTV3i4e7Rt02nN2uW//34oKztrz56ft/7wfY8efbjoDgAAACjnbPTqOyFE/35Dbt66MWHiW8FBIZGRUT269541e5pC4SiE6NWzX/XqNdetX3ny5F+urm5169QbO/Y9IUR4eP0xb09e+e1XGzauEUJENWoyb+6SKlWqCSE6P/fixYvnxk8YPnPGoqLfMvzNsQ4ODh9Nn6zVaoOCQnq/MvCVXv2tN9IAAAAAbILMYDA8YhPr5yQ17exXIUBpoZL+oVarU1KSQ0Or/PMt369au3b5j9sPWvZbHqudy68371YxoIrzfXwWAAAAgA2x3cvM1n+/asiwPpu3rM/MzPjlwJ4NG9e88EIPaxcFAAAAoFyw3avvBvQfkpmZvmfPT19/s8jX179b1559eg+0dlEAAAAAygXbTUpCiFEjJ1q7BAAAAADlke1efQcAAAAA1kJSAgAAAAApkhIAAAAASJGUAAAAAECKpAQAAAAAUiQlAAAAAJAiKQEAAACAFEkJAAAAAKRISgAAAAAgRVICAAAAACkLJCUvX0dhsEQtZY6bl6PcUWbtKgAAAAA8MAskJaWLw92bBZYopqxJiMv2DVJauwoAAAAAD8wCSalKHdeMFJKS1O1r6rAoD8EpJQAAAMAOWSApVQ13dVLKju+9a4l6ygh1ru7QluTWPX2tXQgAAACAhyEzGCxzj9HhH1LVaoNvsHOFIGdZeX1OhEwmy0rV5GRqYw7e7fduZSfn8johAAAAADtnsaQkhLgcm3s5LqewQJ92S2OpNh9Ubm6us7OzXC63yrd7VnQSMkNwNZdGbbytUgAAAAAAi7BkUrIF/fr1mzRpUp06daxdCAAAAAA7xuVhAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABAiqQEAAAAAFIkJQAAAACQIikBAAAAgBRJCQAAAACkSEoAAAAAIEVSAgAAAAApkhIAAAAASJGUAAAAAECKpAQAAAAAUiQlAAAAAJAiKQEAAACAFEkJAAAAAKRISgAAAAAgRVICAAAAACmSEgAAAABIkZQAAAAAQIqkBAAAAABSJCUAAAAAkCIpAQAAAIBUWUtKoaGhDg5lbaQAAAAAlLKyFioSExP1er21qwAAAABg38paUgIAAACAR0dSAgAAAAApkhIAAAAASJGUAAAAAECKpAQAAAAAUiQlAAAAAJAiKQEAAACAFEkJAAAAAKRISgAAAAAgRVICAAAAACmSEgAAAABIkZQAAAAAQIqkBAAAAABSJCUAAAAAkCIpAQAAAICUzGAwWLsGC2jTpo2Dg4ODg0N6erqbm5tcLndwcKhYseKaNWusXRoAAAAA+6OwdgGWoVQqb9++bXydmZkphDAYDF27drV2XQAAAADsUhm5+q5Ro0Z6vb5ol6pVq3bv3t16FQEAAACwY2UkKfXr1y8wMND0ViaTtWrVytfX16pFAQAAALBXZSQpPfHEE1FRUaa3lStX5oQSAAAAgIdWRpKSEGLAgAH+/v7GE0otW7YMCAiwdkUAAAAA7FXZSUpVq1Zt3LixECI0NLRnz57WLgcAAACAHbuPZ98ZRGGhIS9bK2z+ceIvdu576q8LrZ9t5yTzykwttHY59+Agl7l7l5FnDwIAAABlzD3+n9K5v7JO/5aZkaJx9VCUiX+8ZEO8fJ2Sr+XXbOjesgdPngAAAABsi7mkdGJfxu3rBQ1aVXDz4tTHY6HJ16ck5R/ZljJgShWFo8za5QAAAAD4R4lJ6a89aZl3tE2f9yv1ksqdnAztzhVJr31Y1dqFAAAAAPhH8U90yEzVpiQVEJNKh5uXon6LCsf3pVu7EAAAAAD/KD4ppd4sMOhLvZZyzM1Tcf3vPGtXAQAAAOAfxSelnHStbyWXUi+m/PL2U8ocys4T2wEAAAB7V/yjGjQanUZd6rWUY3qDIS25wNpVAAAAAPgH5zEAAAAAQIqkBAAAAABSJCUAAAAAkCIpAQAAAIAUSQkAAAAApEhKAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABAiqQEAAAAAFIkJQAAAACQeuxJ6aWeHb9Z9oVFmkpIuNQqOio29pRFWntQP/28tVV0lFartcq3AwAAAChNnFMCAAAAACmSEgAAAABIKSzVkE6n27hp7berlgoh6tSOGNB/aERE5D/foXDcsvX7JV/Nd3JyCg+PfGfSNE8PTyGEVqtdtnzx0T8Pp6Qkh4dHduvyctOmzxgHycrO+uqrBTt2bvP09Ipq1OT1wSP8/QMk37hq9Tfrvlvx2byltcPqmimspKb++OO3Xw7sjo07lZWVWTss/NVXBzeIjDIOkph4de5n02NjTwUFBj/7bOvXBr7h5ORk7HX3bupH0yfHx8eGhIT26tnvuU5djd3j42O/XbX0/Pl4Ty/vp5o+27/fEFdXV0tNWwAAAAClzGLnlJZ+vWjbto3Tps55b/J0X1//ie+MSEy8auz166F9ubk5M2csGj/ugzNnYlas+NLYfeGiWZs2r+vWtee6tT+2aB49ZeqEXw/tNyaoSe+MTL17Z97cJSPeGp9y5/akySMlNwjt279rxcol77/7ifmYVFJTarV6+qfvFRQUTJo49ZPp80NDq7z73ttpaXeFEMnJt94aMTAiPHLunC979uy3/5ddCxfNMramUCgWfj7r1b6D581dEhZWd/6CGbdvJwshrt9IGjfhTXWB+vNFKz6aOich4e+3xwzhjiYAAADAflnmnFJmVuaGjWtGj5rUOKqpEKJJk6fz8nLvpqWGhlYRQqhUrq/2HWT85JHff42NOyWEKCgo2L3np96vDHihc3chRKeOXc6cOb1q9dctmkcf/fPwuXNnvl2xyTh4pUqVN2xcY4wxRjExJ2bO+nDokJFPP93CfGElNeXn5//N0vUuLi6enl5CiNph4du2b4o7E9OiefSmzeuUzs4DBwyTy+UNGzR2cnK6cOGssTWtVvtC5x5NnmwmhPDzC9i3b+e582f8/QP27dvpqHD8aOocY2vjxr7/Sp/Oh48cbNmijUUmLwAAAIBSZpmkdO1qghAi7H+ndxQKxbSps019I8IjTa89Pbw0BQVCiIsXz2k0msZRT5l6RdZvtHPX9syszMuX/1apVMZsI4SoWSPsvckfCyFycrKFEIlJV5d8NT+6dYdePfvds7CSmhJC5OXlfrPs85jTJ+7eTTV2ychIF0IkJPxdo0aYXC43duzQvnOH9p1NDdav19D4wsvTWwhRoFYLIeLjT4eF1TXGJCFEQEBgUFBIbNwpkhIAAABgpyyTlIwZxlnpXPx3KP7/W2QyWdFBRowaJPlwetrd3NwcZQlNCSEWLJyp1Wp9fCrcT2ElNXX7dvKotwc3bPDk++9+UqdOhEwma9u+qWkQLy/vkho0jYtpRIzjcv7C2VbRUZIRuZ8KAQAAANggyyQlV1c341ma+x+kQkVfIcTYMe8GB1cq2t3PL0Clcs3Pz9Pr9Q4OxdxG1b7d82FhdefOmx4V1bRhg8bmv6Wkpg7+ulej0UyaONXFxcV0Nsk0LrkPMiJCCJ8KFSMiIgcOGFa0o6eH1wM1AgAAAMB2WOaJDtWr11QoFKdjTxrfGgyGSZNH7d79k5lBQoJDlUqlEKJBZJTxr0rlapVDq6pUqrBaddRq9YWL54yfTEy8OnrMkMuX/za+bdf2ueef69b82dbTP3kvMyvTfGElNZWVlenu7mGMSUII45MkjGrVqhMff9r0PIb9v+weN/5NnU5nbvSr1UhJSa5fr6FpXLy9fEyX/AEAAACwO5ZJSm5ubm3bdNq2bePOXdtPxRxf9PnsEyf+rF073MwgKpVqQP+hq1Z/HRcXo9Fofj20f9yEN+cvmCGEiIpqGhxcaenShb8dPnDs+NH5C2bcSblduXLVooNPGD9FoVDMmDnFfGElNVWtWo27d1O3/7hZq9X++dfvJ0/+5enplZKSLIR4rlNXjUYz77NPjp/487fDB77+ZlGFir6m25aK1aNHH71e//niuWq1Oinp2ldLF742uGfClUsPOBUBAAAA2AqL/T+lUSMnzl8wY+686Tqd7onqNad9OPueJ1V69exXvXrNdetXnjz5l6urW9069caOfc94L9CcWYs/nfnBB1PGCyGeeurZTz9ZUPRmJyGEq6vrlPdnvDXytS1bv3+xW88SR6+EpqJbt792LWHV6q8/m/9p46imEyd8uP77Veu+W5mdnTXm7ckzPl04Z85HO3dtVyqV7ds9P3jwW+ZHxMPdY9k3369f/+3QN/omJl4NC6s7ftz7NWuEPeAkBAAAAGArZAaD4b9dj+1Ny88VDVr5WKOk8ig3S7tz+fWBU7hgDwAAALAJFvvPswAAAABQZljs6jtrWffdyu++W1lsr8pVqn2+cHmpVwQAAADA7tl9UurcuXurVu2K7aWQ2/3YAQAAALAKu88S7m7u7m7u1q4CAAAAQJnCfUoAAAAAIEVSAgAAAAApkhIAAAAASJGUAAAAAECKpAQAAAAAUiQlAAAAAJAiKQEAAACAFEkJAAAAAKRISgAAAAAgpSi2q5PSQact9VrKMZlMVjFIae0qAAAAAPyj+HNKHj6OKYn5pV5M+ZWeXGDQG6xdBQAAAIB/FJ+U/EKVMq7LK0XZ6YWhtVTWrgIAAADAP4rPQ64eisphql83JZd6PeXRrYT8iyczI1t6WbsQAAAAAP+QGQwlXvR14URO/B+ZkS0rePk5OSo5x2R5GSmaOzfV545m9J4QKpNZuxoAAAAA/2MuKQkhki7kxfyacTMhXyaXGXR2cCONXq93cLCPUFcxxFmdq6sZ6d64vbe1awEAAADwL/dISiaFGoOwg6AkXn/99bFjx4aFhVm7kHtzkMvkxT96EAAAAICV3e+huqOTfVwcphcauaPBUWkf1QIAAACwTfZxoRoAAAAAlCaSEgAAAABIkZQAAAAAQIqkBAAAAABSJCUAAAAAkCIpAQAAAIAUSQkAAAAApEhKAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABAiqQEAAAAAFIkJQAAAACQIikBAAAAgBRJCQAAAACkSEoAAAAAIEVSAgAAAAApkhIAAAAASJGUAAAAAECKpAQAAAAAUiQlAAAAAJAiKQEAAACAVFlLSpUrV3ZwKGsjBQAAAKCUlbVQce3aNb1eb+0qAAAAANi3spaUAAAAAODRkZQAAAAAQIqkBAAAAABSJCUAAAAAkCIpAQAAAIAUSQkAAAAApEhKAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABAiqQEAAAAAFIkJQAAAACQIikBAAAAgBRJCQAAAACkSEoAAAAAIEVSAgAAAAApmcFgsHYNFtCgQQOZTCaEkMn+GSODwdChQ4dPP/3U2qUBAAAAsD9l5JxSs2bNDAaDg4ODTCZzcHBwcHAIDAx8/fXXrV0XAAAAALtURpLSwIEDvb29i3Zp0qRJtWrVrFcRAAAAADtWRpJSVFRUvXr1TG+DgoL69u1r1YoAAAAA2LEykpSEEAMGDKhQoYLxddOmTatXr27tigAAAADYq7KTlCIjIyMiIoQQISEhPXv2tHY5AAAAAOxY2UlKQog+ffp4eHg0btyYE0oAAAAAHoWFnxJ+Yn/G1bM5DnKHlMR8CzZ7/3Q6nfEJeKX/1W7ejsIggqq7PNnex9VDXvoFAAAAALAUSyal72YnVo/09PF3qhDoXCb+S9ODkclETkZhdlrhHz+ndH0zuEKAk7UrAgAAAPCQLJaU1s1KrN+8QmhtV4u0Zu+2f5nYuqdfYFVnaxcCAAAA4GFYJinFHMzQFMpqP+lpiZLKgoJ8/ZGtyV3eCLJ2IQAAAAAehmWe6HAlPtfLl4vN/p/SxSEzrTDjTqG1CwEAAADwMCyTlGRyWYVApUWaKjMq1XRNu01SAgAAAOySZZJSSpK6/D3B4R7ycnRajc7aVQAAAAB4GGXq/ykBAAAAgEWQlAAAAABAiqQEAAAAAFIkJQAAAACQIikBAAAAgBRJCQAAAACkSEoAAAAAIEVSAgAAAAApkhIAAAAASJGUAAAAAECKpAQAAAAAUiQlAAAAAJAqd0npwMG9raKjMjLSrV0IAAAAANtV7pISAAAAANwTSQkAAAAApBTWLuDB7Nr94/YfN1+5cqlq1Sdat2rX/cVXZDKZEGLqtEkymaxNdMcZsz7Mz8+rUydi2JBRtWuHG4da8tWCPXt/VrmooqM7hIRUtvZIAAAAALB19nROad/+XTNnTa1ZI2zdmu2DBw3ftHnd54vnGnspFIr4s7F79+1Y8uXqnT8fVjopP505xdhr2/ZN27ZvHDVy4uLFqwIDg1et/tqqIwEAAADADthTUtqx44d69RqMHjXJ29unYYPGA/sP++GHDenpaca++Xl548d9EBQYrFAoolt3SEq6lpeXJ4TYsnV9i+ZtWjSP9nD36NC+c8MGja09HgAAAABsnd0kJb1efyb+dOOop0xdGjRorNfrY+NOGd9WCq2iUqmMr93c3IUQ2dlZBoPhxo2kKlWqmYaqWbN2qeLawB4AAAMkSURBVNcOAAAAwM7YzX1KGo2msLBw2fLFy5YvLtrddE7JwaGY1Jebm6vT6VxcVKYuzs4uj79YAAAAAPbNbpKSs7OzSqVq1/a55s2ji3YPCgwxM5Srq6tcLi8oUJu65OfnPc4yAQAAAJQFdpOUhBDVq9fMzsluEBllfFtYWHjr1g0/P38zg8hkMn//wPj4WPHSP12O/nm4NGoFAAAAYM/s5j4lIcTrg946cuTgjp3b9Hp9XFzMtI/eGTNumEajMT9Uq5ZtD/32y4GDe4UQ363/9uzZuNKqFwAAAIC9sqekFBERuXTJ2tjYU926tx034c3c3JyPP5qnVCrND9W3z6DnOnVd9PnsVtFRfxz97c03xgghDAZDaVUNAAAAwP7ILJIZlr6b8OLIKkpne8pdj9uhLbdr1FfVbOhu7UIAAAAAPDCyDQAAAABIWeGJDocPH5w568Nie7m7e2ZnZxbbq1Onrm8MG22pGuLiYia/W3xrOp3OwcFBJpP9t9cLL/R4ffBblqoBAAAAgM2yQlJq2vSZdet+LLaXtrBQ4ehYbC9HRfHdH05ERGRJNZhh2RoAAAAA2CwrJCWFQuHuZv27d2yhBgAAAAC2ifuUAAAAAECKpAQAAAAAUiQlAAAAAJAiKQEAAACAFEkJAAAAAKRISgAAAAAgRVICAAAAACmSEgAAAABIWSYp+fg7OchkFmmqzHBROcjlBFEAAADALlnmUF6nNWTd1VikqTLjdpLa3Udh7SoAAAAAPAzLJKVKNVXZ6YUWaarMcFI6VAhwsnYVAAAAAB6GZZJSs+cr/Lop2SJNlQ2/bbldq5Gb3JErEgEAAAC7JDMYDBZpKC9bt25mYps+QRWClBZp0E5p1Po/fkoJrelS71lPa9cCAAAA4CFZLCkJIfJzdL/9kHrpdHa1CI/stHJ325KLmyLler6Hj2N4M8+wKHdrlwMAAADg4VkyKRnpdYY71wt0Ogs3a/tkQuZRQaHyUPAUQAAAAMDeWT4pAQAAAIC94x/+AAAAAIAUSQkAAAAApEhKAAAAACBFUgIAAAAAKZISAAAAAEiRlAAAAABA6v8AjB/b64TusioAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Comment un agent LangChain choisit-il dynamiquement quel outil appeler lors d'une interaction complexe avec plusieurs intentions utilisateur ?\"\n",
        "source = {\"type\": \"web\", \"path\": \"https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/#add-tools\"}\n",
        "\n",
        "result = executor.invoke(RAGState(query=query, sources=[source]))\n",
        "\n",
        "if(result['cache_hit'] == True) :\n",
        "  display(\"🔁 The query was stored in cache 🔁\")\n",
        "else:\n",
        "  display(\"💬 Generated General Question:\", result['multiple_queries'].general_question)\n",
        "  display(\"🤔 Query Generation Reasoning:\", result['multiple_queries'].reasoning)\n",
        "\n",
        "print(\"📝 Final Answer:\", result['answer'].answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "OA2UvJv_tHsF",
        "outputId": "3d7101c6-5ef5-4725-faac-438baab0b2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Configuration:\n",
            "Model Type: openai\n",
            "Model Name: gpt-4o-mini\n",
            "Temperature: 0.3\n",
            "Max Retries: 3\n",
            "Selected Embedding Configuration:\n",
            "Embedding Type: openai\n",
            "Embedding Model: text-embedding-3-small\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
            "/tmp/ipython-input-12-2745340317.py:4: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  return [loads(doc) for doc in unique_docs]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Configuration:\n",
            "Model Type: openai\n",
            "Model Name: gpt-4o-mini\n",
            "Temperature: 0.3\n",
            "Max Retries: 3\n",
            "💬 Réponse : answer=\"Un agent intelligent gère la sélection d'outils en définissant un graphe qui représente les différentes intentions des utilisateurs et les outils disponibles. Lorsqu'un utilisateur pose une question, l'agent évalue les intentions multiples en utilisant des conditions prédéfinies (conditional_edges) pour déterminer quel outil est le plus approprié à utiliser. De plus, l'agent peut être configuré pour ajouter des composants préconstruits et personnaliser l'état afin de mieux répondre aux besoins spécifiques des utilisateurs. Cela permet une interaction dynamique et efficace, où l'agent peut adapter ses réponses et ses actions en fonction des intentions détectées.\" sources=['Agent development', 'Workflows & agents', 'Prebuilt components']\n",
            "📝 Entrez un feedback (ou laissez vide pour continuer) : \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langgraph:Task feedback with path ('__pregel_pull', 'feedback') wrote to unknown channel branch:to:end, ignoring it.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'💬 Generated General Question:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Comment un agent intelligent gère-t-il la sélection d'outils en fonction des intentions multiples des utilisateurs ?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'🤔 Query Generation Reasoning:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"La question initiale se concentre sur un aspect spécifique de LangChain, mais elle peut être généralisée pour inclure d'autres systèmes d'agents intelligents qui doivent également gérer des interactions complexes et des intentions multiples.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Final Answer: Un agent intelligent gère la sélection d'outils en définissant un graphe qui représente les différentes intentions des utilisateurs et les outils disponibles. Lorsqu'un utilisateur pose une question, l'agent évalue les intentions multiples en utilisant des conditions prédéfinies (conditional_edges) pour déterminer quel outil est le plus approprié à utiliser. De plus, l'agent peut être configuré pour ajouter des composants préconstruits et personnaliser l'état afin de mieux répondre aux besoins spécifiques des utilisateurs. Cela permet une interaction dynamique et efficace, où l'agent peut adapter ses réponses et ses actions en fonction des intentions détectées.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyDE et Wikipédia"
      ],
      "metadata": {
        "id": "ophk1sTHLr8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Maintenant, nous allons utiliser un retriever basé sur Wikipedia pour récupérer des informations réelles après avoir généré un document hypothétique avec l'approche HyDE.\n",
        "\n",
        "---\n",
        "##🏗 Pourquoi utiliser Wikipedia comme Retriever ?\n",
        "\n",
        "* ✅ Données riches et vérifiées : Wikipedia est une source fiable pour des faits et concepts.\n",
        "* ✅ Large couverture : Idéal pour tester l’amélioration de la récupération d’information avec HyDE.\n",
        "* ✅ Accès rapide : Via l’API LangChain, nous pouvons interroger Wikipedia efficacement.\n",
        "---\n",
        "\n",
        "## 🛠 Plan d'implémentation\n",
        "* 1️⃣ Génération d’un document hypothétique avec un modèle de langage (HyDE).\n",
        "* 2️⃣ Utilisation de Wikipedia Retriever pour chercher des documents correspondants à ce document.\n",
        "* 3️⃣ Comparaison des résultats obtenus avec et sans HyDE.\n",
        "* 4️⃣ Génération de la réponse finale en utilisant RAG."
      ],
      "metadata": {
        "id": "T-KghcmpLyXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcZiX7Kw5v52",
        "outputId": "8a092800-45aa-463c-ec05-0b252d5db331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.documents import Document\n",
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph\n",
        "from operator import itemgetter\n",
        "from langchain.load import dumps, loads\n",
        "from functools import lru_cache\n",
        "from diskcache import Cache\n",
        "from langsmith import traceable, Client\n",
        "import json"
      ],
      "metadata": {
        "id": "yjC1GTT7T0ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.retrievers import WikipediaRetriever\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.documents import Document # Import Document\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain # Import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain # Import create_retrieval_chain\n",
        "\n",
        "# ----------- 🧠 State Definition -----------\n",
        "\n",
        "class GraphState(TypedDict, total=False):\n",
        "    question: str\n",
        "    hypothetical_doc: str\n",
        "    retrieved_hyde: List[Document] # Changed to List[Document]\n",
        "    retrieved_direct: List[Document] # Changed to List[Document]\n",
        "    final_answer: str\n",
        "    use_hyde: bool\n",
        "    show_docs: bool\n",
        "\n",
        "\n",
        "# ----------- 🔧 Components -----------\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, api_key=OPENAI_API_KEY)\n",
        "retriever = WikipediaRetriever()\n",
        "\n",
        "template = \"\"\"Rewrite the question into a more complete and more detailed question on the disscussed matter. Don't add extra words like 'Wikipedia' or quotes:\n",
        "Question: {question}\n",
        "Search Query:\"\"\"\n",
        "\n",
        "\n",
        "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
        "hyde_chain = prompt_hyde | llm | StrOutputParser()\n",
        "\n",
        "# 🔹 2. RAG Chain (Réutilisé plus tard)\n",
        "# Define a simple prompt for the RAG chain\n",
        "qa_prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\")\n",
        "\n",
        "# Create a document combining chain\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "\n",
        "# ----------- ⚙️ Functions -----------\n",
        "\n",
        "def generate_hypothetical_doc(state: GraphState):\n",
        "    hypo = hyde_chain.invoke({\"question\": state[\"question\"]})\n",
        "    print(f\"\\n🔹 Generated Search Query from HyDE: {hypo}\\n\")\n",
        "    return {**state, \"hypothetical_doc\": hypo}\n",
        "\n",
        "def print_documents(docs: List[Document], label: str):\n",
        "    print(f\"\\n📚 {label} Documents Retrieved ({len(docs)} documents):\")\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "      print(f\"\\n--- Document {i} ---\")\n",
        "      print(\"Content Preview:\", doc.page_content)\n",
        "      if doc.metadata:\n",
        "          print(\"Metadata:\", doc.metadata)\n",
        "\n",
        "\n",
        "def retrieve_with_hyde(state: GraphState):\n",
        "    show_docs = state.get(\"show_docs\", True)\n",
        "    query = state[\"hypothetical_doc\"]\n",
        "    query = query.strip().strip('\"').replace(\"Wikipedia\", \"\").strip()\n",
        "    print(f\"🔹 Cleaned HyDE Query: {query}\")\n",
        "    docs = retriever.invoke(query)\n",
        "    if show_docs:\n",
        "        print_documents(docs, \"HyDE\")\n",
        "    return {**state, \"retrieved_hyde\": docs}\n",
        "\n",
        "def retrieve_direct(state: GraphState):\n",
        "    show_docs = state.get(\"show_docs\", True)\n",
        "    docs = retriever.invoke(state[\"question\"])\n",
        "    if show_docs:\n",
        "        print_documents(docs, \"Direct\")\n",
        "    return {**state, \"retrieved_direct\": docs}\n",
        "\n",
        "\n",
        "\n",
        "def compare_results(state: GraphState):\n",
        "    len_hyde = len(state.get(\"retrieved_hyde\", []))\n",
        "    len_direct = len(state.get(\"retrieved_direct\", []))\n",
        "    print(f\"📊 Wikipedia Results: HyDE = {len_hyde}, Direct = {len_direct}\")\n",
        "    return state\n",
        "\n",
        "def generate_final_answer(state: GraphState):\n",
        "    label = \"RAG + HyDE\" if state.get(\"use_hyde\") else \"Direct RAG\"\n",
        "\n",
        "    if state.get(\"use_hyde\"):\n",
        "        # Use retrieved documents from HyDE\n",
        "        response = combine_docs_chain.invoke({\"question\": state[\"question\"], \"context\": state[\"retrieved_hyde\"]})\n",
        "    else:\n",
        "        # Use retrieved documents from direct retrieval\n",
        "        response = combine_docs_chain.invoke({\"question\": state[\"question\"], \"context\": state[\"retrieved_direct\"]})\n",
        "\n",
        "\n",
        "    print(f\"\\n🧠 Final Answer ({label}):\\n{response}\")\n",
        "    return {**state, \"final_answer\": response}\n",
        "\n"
      ],
      "metadata": {
        "id": "gYvu2PynQR9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-sNg9o2fQWAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph WITH HyDE\n",
        "\n",
        "builder_hyde = StateGraph(GraphState)\n",
        "builder_hyde.add_node(\"generate_hypothetical_doc\", RunnableLambda(generate_hypothetical_doc))\n",
        "builder_hyde.add_node(\"retrieve_with_hyde\", RunnableLambda(retrieve_with_hyde))\n",
        "builder_hyde.add_node(\"retrieve_direct\", RunnableLambda(retrieve_direct))\n",
        "builder_hyde.add_node(\"compare_results\", RunnableLambda(compare_results))\n",
        "builder_hyde.add_node(\"generate_final_answer\", RunnableLambda(generate_final_answer))\n",
        "\n",
        "builder_hyde.set_entry_point(\"generate_hypothetical_doc\")\n",
        "builder_hyde.add_edge(\"generate_hypothetical_doc\", \"retrieve_with_hyde\")\n",
        "builder_hyde.add_edge(\"retrieve_with_hyde\", \"retrieve_direct\")\n",
        "builder_hyde.add_edge(\"retrieve_direct\", \"compare_results\")\n",
        "builder_hyde.add_edge(\"compare_results\", \"generate_final_answer\")\n",
        "builder_hyde.add_edge(\"generate_final_answer\", '__end__')\n",
        "\n",
        "graph_hyde = builder_hyde.compile()\n"
      ],
      "metadata": {
        "id": "vQPNuyhOOvX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph WITHOUT HyDE\n",
        "builder_direct = StateGraph(GraphState)\n",
        "builder_direct.add_node(\"retrieve_direct\", RunnableLambda(retrieve_direct))\n",
        "builder_direct.add_node(\"generate_final_answer\", RunnableLambda(generate_final_answer))\n",
        "\n",
        "builder_direct.set_entry_point(\"retrieve_direct\")\n",
        "builder_direct.add_edge(\"retrieve_direct\", \"generate_final_answer\")\n",
        "builder_direct.add_edge(\"generate_final_answer\", '__end__')\n",
        "\n",
        "graph_direct = builder_direct.compile()"
      ],
      "metadata": {
        "id": "bCawjCDJOha5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions = [\n",
        "    \"What are the basic rules of chess and how does each piece move?\",\n",
        "    \"What strategies can beginners use to improve their chess game?\",\n",
        "    \"What are common checkmate patterns every player should know?\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n=== Testing question: {question} ===\\n\")\n",
        "\n",
        "    state = GraphState(question=question, use_hyde=True, show_docs=False)\n",
        "    result_hyde = graph_hyde.invoke(state)\n",
        "\n",
        "    state = GraphState(question=question, use_hyde=False, show_docs=False)\n",
        "    result_direct = graph_direct.invoke(state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0Jy1Bw3O923",
        "outputId": "564c6a8a-6c8a-412a-88e6-e42ebe3fa4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing question: What are the basic rules of chess and how does each piece move? ===\n",
            "\n",
            "\n",
            "🔹 Generated Search Query from HyDE: What are the fundamental rules of chess, including how each individual piece moves on the board?\n",
            "\n",
            "🔹 Cleaned HyDE Query: What are the fundamental rules of chess, including how each individual piece moves on the board?\n",
            "📊 Wikipedia Results: HyDE = 3, Direct = 3\n",
            "\n",
            "🧠 Final Answer (RAG + HyDE):\n",
            "The basic rules of chess involve moving pieces on a board with the goal of checkmating the opponent's king. Each piece has its own unique way of moving:\n",
            "\n",
            "1. Pawn: Pawns move forward one square, but capture diagonally. On their first move, pawns have the option to move two squares forward. Pawns can also promote to any other piece (except a king) if they reach the opponent's back rank.\n",
            "\n",
            "2. Rook: Rooks move horizontally or vertically as many squares as they want.\n",
            "\n",
            "3. Knight: Knights move in an L-shape, two squares in one direction and then one square perpendicular to that.\n",
            "\n",
            "4. Bishop: Bishops move diagonally as many squares as they want.\n",
            "\n",
            "5. Queen: Queens combine the movement of rooks and bishops, being able to move horizontally, vertically, or diagonally as many squares as they want.\n",
            "\n",
            "6. King: Kings move one square in any direction. The king cannot move into check.\n",
            "\n",
            "These are the basic rules of how each piece moves in chess.\n",
            "\n",
            "🧠 Final Answer (Direct RAG):\n",
            "The basic rules of chess include moving pieces on a board with the goal of checkmating the opponent's king. Each player starts with 16 pieces: one king, one queen, two rooks, two knights, two bishops, and eight pawns. \n",
            "\n",
            "- The king can move one square in any direction.\n",
            "- The queen can move any number of squares in any direction.\n",
            "- The rook can move any number of squares horizontally or vertically.\n",
            "- The bishop can move any number of squares diagonally.\n",
            "- The knight moves in an L-shape, two squares in one direction and then one square perpendicular.\n",
            "- The pawn moves forward one square, but captures diagonally. On its first move, a pawn can move two squares forward. Pawns can also promote to any other piece if they reach the opposite end of the board.\n",
            "\n",
            "Each piece has its own unique way of moving and capturing, and the objective is to use these pieces strategically to control the board and ultimately checkmate the opponent's king.\n",
            "\n",
            "=== Testing question: What strategies can beginners use to improve their chess game? ===\n",
            "\n",
            "\n",
            "🔹 Generated Search Query from HyDE: Effective strategies for beginners to enhance their skills in chess.\n",
            "\n",
            "🔹 Cleaned HyDE Query: Effective strategies for beginners to enhance their skills in chess.\n",
            "📊 Wikipedia Results: HyDE = 3, Direct = 3\n",
            "\n",
            "🧠 Final Answer (RAG + HyDE):\n",
            "Beginners can improve their chess game by practicing regularly, studying basic tactics and strategies, analyzing their games to learn from mistakes, playing against stronger opponents to challenge themselves, and seeking guidance from experienced players or chess coaches. Additionally, beginners can benefit from learning common opening principles, developing a solid understanding of endgame techniques, and familiarizing themselves with key chess concepts such as controlling the center, piece development, and pawn structure.\n",
            "\n",
            "🧠 Final Answer (Direct RAG):\n",
            "Beginners can improve their chess game by focusing on the following strategies:\n",
            "1. Learn basic opening principles and common opening sequences.\n",
            "2. Practice tactics such as forks, pins, skewers, and discovered attacks.\n",
            "3. Study endgame principles and practice endgame scenarios.\n",
            "4. Analyze games to understand mistakes and learn from them.\n",
            "5. Play regularly against opponents of varying skill levels to gain experience.\n",
            "6. Utilize chess software and engines for analysis and practice.\n",
            "7. Study famous games and grandmaster games to understand strategic concepts.\n",
            "8. Develop a solid understanding of pawn structure and pawn play.\n",
            "9. Work on improving calculation skills and visualization of the board.\n",
            "10. Seek feedback and guidance from more experienced players or coaches.\n",
            "\n",
            "=== Testing question: What are common checkmate patterns every player should know? ===\n",
            "\n",
            "\n",
            "🔹 Generated Search Query from HyDE: Common checkmate patterns every player should know\n",
            "\n",
            "🔹 Cleaned HyDE Query: Common checkmate patterns every player should know\n",
            "📊 Wikipedia Results: HyDE = 3, Direct = 3\n",
            "\n",
            "🧠 Final Answer (RAG + HyDE):\n",
            "Common checkmate patterns every player should know include:\n",
            "\n",
            "1. Back rank mate\n",
            "2. Anastasia's mate\n",
            "3. Arabian mate\n",
            "4. Boden's mate\n",
            "5. Damiano's mate\n",
            "6. Fool's mate\n",
            "7. Greco's mate\n",
            "8. Legall's mate\n",
            "9. Lolli's mate\n",
            "10. Morphy's mate\n",
            "\n",
            "These are just a few examples of checkmate patterns that players should be familiar with in order to improve their chess skills.\n",
            "\n",
            "🧠 Final Answer (Direct RAG):\n",
            "Common checkmate patterns every player should know include back rank mate, smothered mate, and the two-rook checkmate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparaison et Analyse des résultats\n",
        "\n",
        "### Analyse question par question\n",
        "\n",
        "##### \"What are the basic rules of chess and how does each piece move?\"\n",
        "\n",
        "* Avec HyDE :\n",
        "\n",
        "La requête générée est plus détaillée et précise, avec une formulation enrichie :\n",
        "« What are the fundamental rules of chess, including how each individual piece moves on the board? »\n",
        "\n",
        "La réponse finale est claire, complète et bien structurée, avec des explications spécifiques pour chaque pièce et des mentions importantes (promotion des pions, impossibilité de déplacer le roi en échec).\n",
        "\n",
        " * Sans HyDE (direct) :\n",
        "\n",
        "La requête initiale est utilisée telle quelle, plus simple.\n",
        "\n",
        "La réponse est aussi complète, même un peu plus longue, ajoutant la composition initiale des pièces.\n",
        "\n",
        "L’explication reste détaillée, mais légèrement plus générique, moins fluide.\n",
        "\n",
        "* Pertinence & Qualité :\n",
        "\n",
        "Les deux réponses sont pertinentes et exactes. HyDE permet d’affiner la requête et d’obtenir un texte plus fluide, focalisé et bien organisé. Sans HyDE, la réponse est correcte mais un peu plus brute.\n",
        "\n",
        "---\n",
        "\n",
        "##### \"What strategies can beginners use to improve their chess game?\"\n",
        "\n",
        "* Avec HyDE :\n",
        "\n",
        "La requête est reformulée vers « stratégies efficaces pour débutants ».\n",
        "\n",
        "La réponse est concise, claire, avec une bonne synthèse des conseils essentiels : pratique, étude des tactiques, analyse, jeu contre plus forts, apprentissage des concepts clés (centre, structure de pions).\n",
        "\n",
        "* Sans HyDE (direct) :\n",
        "\n",
        "La réponse est plus détaillée, liste dix conseils précis allant de l’étude des ouvertures à l’analyse des parties et au recours aux logiciels.\n",
        "\n",
        "Plus exhaustive, mais potentiellement moins accessible pour un débutant absolu.\n",
        "\n",
        "* Pertinence & Qualité :\n",
        "\n",
        "La réponse avec HyDE privilégie la simplicité et la clarté, adaptée à un débutant qui cherche un bon point de départ. La réponse directe est plus complète, utile pour un utilisateur recherchant une liste exhaustive. L’usage de HyDE oriente donc la génération vers une réponse plus ciblée.\n",
        "\n",
        "---\n",
        "\n",
        "##### \"What are common checkmate patterns every player should know?\"\n",
        "\n",
        "* Avec HyDE :\n",
        "La requête reformulée cible directement les motifs de mat.\n",
        "\n",
        "La réponse énumère 10 motifs de mat spécifiques, donnant des noms connus et reconnus dans la communauté échiquéenne (Back rank mate, Anastasia’s mate, etc.).\n",
        "\n",
        "* Sans HyDE (direct) :\n",
        "\n",
        "La réponse est plus courte et mentionne seulement 3 motifs (Back rank, smothered, two-rook).\n",
        "\n",
        "Moins complète et moins variée.\n",
        "\n",
        "* Pertinence & Qualité :\n",
        "\n",
        "La réponse HyDE est clairement plus riche, donnant un panorama plus large des motifs de mat. La reformulation améliore la qualité et la richesse du contenu récupéré et généré.\n"
      ],
      "metadata": {
        "id": "qzl5UPbzg9T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse globale et conclusions\n",
        "\n",
        "* Avantages de HyDE :\n",
        "\n",
        "Génération automatique de requêtes plus précises et naturelles qui capturent mieux l’intention de la question.\n",
        "\n",
        "Recherche d’informations plus ciblée dans Wikipedia, ce qui améliore la qualité et la richesse des extraits.\n",
        "\n",
        "Réponses finales mieux structurées, plus complètes et pertinentes, souvent plus fluides et synthétiques.\n",
        "\n",
        "* Limites sans HyDE :\n",
        "\n",
        "Le retriever traite la question brute, parfois trop simple ou vague, ce qui peut conduire à des résultats moins optimaux.\n",
        "\n",
        "Réponses plus brutes, parfois moins bien organisées ou moins exhaustives.\n",
        "\n",
        "* Pertinence du RAG avec HyDE :\n",
        "\n",
        "RAG s’appuie sur un corpus récupéré plus pertinent, ce qui améliore la génération.\n",
        "\n",
        "Les réponses sont plus factuelles, adaptées au contexte de la question, et plus détaillées.\n",
        "\n",
        "HyDE agit comme un « reformulateur » améliorant la phase de récupération d’information.\n"
      ],
      "metadata": {
        "id": "wSo-ToVZiiPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Routing"
      ],
      "metadata": {
        "id": "JrcLCpEEOm94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RoutingState(TypedDict, total=False):\n",
        "    query: str\n",
        "    selected_prompt: str\n",
        "    confirmed_prompt: str\n",
        "    user_choice: str\n",
        "    final_answer: str\n",
        "\n"
      ],
      "metadata": {
        "id": "izziXpBxOny0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, api_key=OPENAI_API_KEY)\n",
        "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
        "\n",
        "archaeologist_template = \"\"\"Vous êtes un archéologue renommé spécialisé dans les civilisations anciennes.\n",
        "Vous répondez aux questions sur l'histoire, les sites archéologiques, et les découvertes historiques avec des explications précises et fascinantes.\n",
        "Si la question ne fait pas partie de votre spécialité ou est hors sujet, vous répondrez \"Je ne sais pas\" sans explication détaillée.\n",
        "\n",
        "Voici une question :\n",
        "{query}\"\"\"\n",
        "\n",
        "doctor_template = \"\"\"Vous êtes un médecin expert en biologie et en santé.\n",
        "Vous donnez des explications détaillées sur les maladies, les traitements et le fonctionnement du corps humain.\n",
        "Vous basez vos réponses sur des faits scientifiques et des études médicales reconnues.\n",
        "Si la question ne fait pas partie de votre spécialité ou est hors sujet, vous répondrez \"Je ne sais pas\" sans explication détaillée.\n",
        "Voici une question :\n",
        "{query}\"\"\"\n",
        "\n",
        "ai_engineer_template = \"\"\"Vous êtes un ingénieur en intelligence artificielle et en automatisation.\n",
        "Vous êtes spécialisé dans les réseaux de neurones, l'apprentissage automatique et les modèles de langage.\n",
        "Vous expliquez les concepts d’IA de manière claire et pédagogique.\n",
        "Si la question ne fait pas partie de votre spécialité ou est hors sujet, vous répondrez \"Je ne sais pas\" sans explication détaillée.\n",
        "Voici une question :\n",
        "{query}\"\"\"\n",
        "\n",
        "prompt_templates = [archaeologist_template, doctor_template, ai_engineer_template]\n",
        "expert_names = [\"archaeologist\", \"doctor\", \"ai_engineer\"]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
        "\n",
        "prompt_name_map = {\n",
        "    archaeologist_template: \"archaeologist\",\n",
        "    doctor_template: \"doctor\",\n",
        "    ai_engineer_template: \"ai_engineer\"\n",
        "}"
      ],
      "metadata": {
        "id": "eIB0H88jOqGf"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def route_expert(state: RoutingState) -> RoutingState:\n",
        "    query = state[\"query\"]\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    similarity_scores = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "    ranked_indices = np.argsort(similarity_scores)[::-1]\n",
        "    best_idx, second_idx = ranked_indices[0], ranked_indices[1]\n",
        "    top1_score, top2_score = similarity_scores[best_idx], similarity_scores[second_idx]\n",
        "    top1_prompt = prompt_templates[best_idx]\n",
        "    top2_prompt = prompt_templates[second_idx]\n",
        "\n",
        "    for i, score in enumerate(similarity_scores):\n",
        "      print(f\"🔎 {prompt_name_map[prompt_templates[i]]}: {score:.4f}\")\n",
        "\n",
        "    print(f\"\\n🔍 Similarité max : {top1_score:.4f} | 2e plus proche : {top2_score:.4f}\")\n",
        "    top2_diff = abs(top1_score - top2_score)\n",
        "\n",
        "    if top2_diff < 0.01:\n",
        "        print(\"👥 Plusieurs experts semblent pertinents pour cette question :\")\n",
        "        print(f\"  1️⃣ {prompt_name_map[top1_prompt]} (score: {top1_score:.4f})\")\n",
        "        print(f\"  2️⃣ {prompt_name_map[top2_prompt]} (score: {top2_score:.4f})\")\n",
        "        print(\"Vous pourrez choisir entre les deux à l'étape suivante.\")\n",
        "        suggested_prompt = top1_prompt\n",
        "        state = {\n",
        "            **state,\n",
        "            \"selected_prompt\": suggested_prompt,\n",
        "            \"alt_prompt\": top2_prompt,\n",
        "            \"is_interdisciplinary\": True,\n",
        "        }\n",
        "    else:\n",
        "        print(f\"✅ Expert sélectionné automatiquement : {prompt_name_map[top1_prompt]}\")\n",
        "        state = {\n",
        "            **state,\n",
        "            \"selected_prompt\": top1_prompt,\n",
        "            \"is_interdisciplinary\": False,\n",
        "        }\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "6OK7n8vLOti2"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_affinage(state: RoutingState) -> RoutingState:\n",
        "    if state.get(\"is_interdisciplinary\", False):\n",
        "        prompt1 = state[\"selected_prompt\"]\n",
        "        prompt2 = state[\"alt_prompt\"]\n",
        "\n",
        "        print(\"\\n👥 Deux experts sont proches :\")\n",
        "        print(f\"  1️⃣ {prompt_name_map[prompt1]}\")\n",
        "        print(f\"  2️⃣ {prompt_name_map[prompt2]}\")\n",
        "        print(\"Choisissez celui que vous préférez (1 ou 2), ou tapez autre nom d’expert manuellement.\")\n",
        "        print(\"Tapez 'auto' pour garder le 1er automatiquement.\")\n",
        "\n",
        "        user_input = input(\"👉 Votre choix : \").strip().lower()\n",
        "\n",
        "        if user_input == \"1\" or user_input == \"auto\":\n",
        "            confirmed_prompt = prompt1\n",
        "        elif user_input == \"2\":\n",
        "            confirmed_prompt = prompt2\n",
        "        elif user_input in prompt_name_map.values():\n",
        "            for p in prompt_templates:\n",
        "                if prompt_name_map[p] == user_input:\n",
        "                    confirmed_prompt = p\n",
        "                    break\n",
        "            else:\n",
        "                print(\"⚠️ Entrée invalide, expert automatique conservé.\")\n",
        "                confirmed_prompt = prompt1\n",
        "        else:\n",
        "            print(\"⚠️ Entrée invalide, expert automatique conservé.\")\n",
        "            confirmed_prompt = prompt1\n",
        "\n",
        "    else:\n",
        "        selected_prompt = state[\"selected_prompt\"]\n",
        "        selected_name = prompt_name_map[selected_prompt]\n",
        "        print(f\"\\n🤖 L'expert sélectionné automatiquement est : {selected_name}\")\n",
        "        print(\"Souhaitez-vous le confirmer ou choisir un autre expert ?\")\n",
        "        print(\"Options : auto / doctor / ai_engineer / archaeologist\")\n",
        "        user_input = input(\"👉 Votre choix : \").strip().lower()\n",
        "\n",
        "        if user_input == \"auto\" or user_input == selected_name:\n",
        "            confirmed_prompt = selected_prompt\n",
        "        else:\n",
        "            for p in prompt_templates:\n",
        "                if prompt_name_map[p] == user_input:\n",
        "                    confirmed_prompt = p\n",
        "                    break\n",
        "            else:\n",
        "                print(\"⚠️ Entrée invalide, expert automatique conservé.\")\n",
        "                confirmed_prompt = selected_prompt\n",
        "\n",
        "    print(f\"✅ Expert confirmé : {prompt_name_map[confirmed_prompt]}\")\n",
        "    return {**state, \"confirmed_prompt\": confirmed_prompt, \"user_choice\": user_input}\n"
      ],
      "metadata": {
        "id": "13Br9TPjVQ9H"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confirm_expert(state: RoutingState) -> RoutingState:\n",
        "    confirmed = state[\"selected_prompt\"]\n",
        "    print(f\"✅ Expert confirmé : {prompt_name_map[confirmed]}\")\n",
        "    return {**state, \"confirmed_prompt\": confirmed}\n"
      ],
      "metadata": {
        "id": "CnYRiiVIOwEL"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(state: RoutingState) -> RoutingState:\n",
        "    prompt_template = PromptTemplate.from_template(state[\"confirmed_prompt\"])\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "    answer = chain.invoke({\"query\": state[\"query\"]})\n",
        "\n",
        "    print(f\"🧠 Réponse générée :\\n{answer}\")\n",
        "    return {**state, \"final_answer\": answer}\n"
      ],
      "metadata": {
        "id": "VPHej_gsOyHn"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "builder = StateGraph(RoutingState)\n",
        "\n",
        "builder.add_node(\"route_expert\", RunnableLambda(route_expert))\n",
        "builder.add_node(\"user_affinage\", RunnableLambda(user_affinage))\n",
        "builder.add_node(\"generate_response\", RunnableLambda(generate_response))\n",
        "\n",
        "builder.set_entry_point(\"route_expert\")\n",
        "builder.add_edge(\"route_expert\", \"user_affinage\")\n",
        "builder.add_edge(\"user_affinage\", \"generate_response\")\n",
        "builder.add_edge(\"generate_response\", \"__end__\")\n",
        "\n",
        "graph_with_affinage = builder.compile()\n"
      ],
      "metadata": {
        "id": "f7Pyv_tvOzxw"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions = [\n",
        "    \"Comment l’intelligence artificielle influence-t-elle la médecine moderne ?\",\n",
        "    \"Quelles sont les grandes découvertes de l'Égypte antique ?\",\n",
        "    \"Quels sont les effets secondaires du vaccin contre la grippe ?\",\n",
        "    \"Quels sont les avantages de l’apprentissage profond dans le traitement du langage naturel ?\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n=== Question: {question} ===\\n\")\n",
        "    state = RoutingState(query=question)\n",
        "    result = graph_with_affinage.invoke(state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Anhfz21tSnlm",
        "outputId": "cfda5c03-ac80-4a07-98b9-8ee30f693987"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Question: Comment l’intelligence artificielle influence-t-elle la médecine moderne ? ===\n",
            "\n",
            "🔎 archaeologist: 0.7592\n",
            "🔎 doctor: 0.8016\n",
            "🔎 ai_engineer: 0.8115\n",
            "\n",
            "🔍 Similarité max : 0.8115 | 2e plus proche : 0.8016\n",
            "👥 Plusieurs experts semblent pertinents pour cette question :\n",
            "  1️⃣ ai_engineer (score: 0.8115)\n",
            "  2️⃣ doctor (score: 0.8016)\n",
            "Vous pourrez choisir entre les deux à l'étape suivante.\n",
            "\n",
            "🤖 L'expert sélectionné automatiquement est : ai_engineer\n",
            "Souhaitez-vous le confirmer ou choisir un autre expert ?\n",
            "Options : auto / doctor / ai_engineer / archaeologist\n",
            "👉 Votre choix : ai_engineer\n",
            "✅ Expert confirmé : ai_engineer\n",
            "🧠 Réponse générée :\n",
            "L'intelligence artificielle a un impact significatif sur la médecine moderne en permettant des avancées majeures dans de nombreux domaines. Par exemple, les réseaux de neurones peuvent être utilisés pour analyser de grandes quantités de données médicales et identifier des schémas ou des corrélations qui échapperaient à l'œil humain. Cela peut aider les médecins à diagnostiquer plus rapidement et plus précisément certaines maladies, à prédire l'évolution des patients ou à recommander des traitements personnalisés. De plus, l'automatisation des tâches répétitives ou chronophages grâce à l'IA permet aux professionnels de santé de se concentrer davantage sur les aspects humains et décisionnels de leur métier. Enfin, les modèles de langage peuvent être utilisés pour analyser et interpréter des documents médicaux, contribuant ainsi à une meilleure gestion des données et à une recherche plus efficace.\n",
            "\n",
            "=== Question: Quelles sont les grandes découvertes de l'Égypte antique ? ===\n",
            "\n",
            "🔎 archaeologist: 0.8075\n",
            "🔎 doctor: 0.7513\n",
            "🔎 ai_engineer: 0.7401\n",
            "\n",
            "🔍 Similarité max : 0.8075 | 2e plus proche : 0.7513\n",
            "✅ Expert sélectionné automatiquement : archaeologist\n",
            "\n",
            "🤖 L'expert sélectionné automatiquement est : archaeologist\n",
            "Souhaitez-vous le confirmer ou choisir un autre expert ?\n",
            "Options : auto / doctor / ai_engineer / archaeologist\n",
            "👉 Votre choix : doctor\n",
            "✅ Expert confirmé : doctor\n",
            "🧠 Réponse générée :\n",
            "Je suis désolé, mais cette question ne relève pas de ma spécialité en tant que médecin expert en biologie et en santé. Je ne peux donc pas fournir de réponse détaillée à ce sujet.\n",
            "\n",
            "=== Question: Quels sont les effets secondaires du vaccin contre la grippe ? ===\n",
            "\n",
            "🔎 archaeologist: 0.7186\n",
            "🔎 doctor: 0.7559\n",
            "🔎 ai_engineer: 0.7251\n",
            "\n",
            "🔍 Similarité max : 0.7559 | 2e plus proche : 0.7251\n",
            "✅ Expert sélectionné automatiquement : doctor\n",
            "\n",
            "🤖 L'expert sélectionné automatiquement est : doctor\n",
            "Souhaitez-vous le confirmer ou choisir un autre expert ?\n",
            "Options : auto / doctor / ai_engineer / archaeologist\n",
            "👉 Votre choix : doctor\n",
            "✅ Expert confirmé : doctor\n",
            "🧠 Réponse générée :\n",
            "Les effets secondaires les plus courants du vaccin contre la grippe sont généralement légers et temporaires. Ils peuvent inclure une légère douleur, rougeur ou enflure au site d'injection, de la fièvre, des maux de tête, de la fatigue ou des courbatures. Ces effets secondaires disparaissent généralement en quelques jours.\n",
            "\n",
            "Il est important de noter que les avantages de la vaccination contre la grippe l'emportent largement sur les risques potentiels d'effets secondaires. La vaccination est un moyen efficace de prévenir la grippe et ses complications, en particulier chez les personnes à risque de complications graves, comme les personnes âgées, les jeunes enfants, les femmes enceintes et les personnes souffrant de certaines conditions médicales.\n",
            "\n",
            "Il est recommandé de consulter un professionnel de la santé pour obtenir des informations personnalisées sur la vaccination contre la grippe et pour discuter de tout risque potentiel d'effets secondaires.\n",
            "\n",
            "=== Question: Quels sont les avantages de l’apprentissage profond dans le traitement du langage naturel ? ===\n",
            "\n",
            "🔎 archaeologist: 0.7662\n",
            "🔎 doctor: 0.7748\n",
            "🔎 ai_engineer: 0.7917\n",
            "\n",
            "🔍 Similarité max : 0.7917 | 2e plus proche : 0.7748\n",
            "✅ Expert sélectionné automatiquement : ai_engineer\n",
            "\n",
            "🤖 L'expert sélectionné automatiquement est : ai_engineer\n",
            "Souhaitez-vous le confirmer ou choisir un autre expert ?\n",
            "Options : auto / doctor / ai_engineer / archaeologist\n",
            "👉 Votre choix : auto\n",
            "✅ Expert confirmé : ai_engineer\n",
            "🧠 Réponse générée :\n",
            "L'apprentissage profond présente plusieurs avantages dans le traitement du langage naturel. Tout d'abord, les réseaux de neurones profonds permettent de capturer des informations complexes et subtiles dans les données textuelles, ce qui améliore la précision des modèles de langage. Ensuite, l'apprentissage profond permet d'extraire automatiquement des caractéristiques pertinentes des données textuelles, ce qui évite la nécessité de concevoir manuellement des règles spécifiques pour chaque tâche de traitement du langage naturel. De plus, les modèles de langage profonds peuvent être entraînés sur de grandes quantités de données, ce qui améliore leur capacité à généraliser et à traiter efficacement différents types de textes. Enfin, l'apprentissage profond permet de créer des modèles de langage plus flexibles et adaptables, capables de s'adapter à de nouveaux types de données et de tâches de manière plus efficace que les approches traditionnelles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Comparaison des résultats selon les questions\n",
        "\n",
        "### 1. Question : Comment l’intelligence artificielle influence-t-elle la médecine moderne ?\n",
        "Scores :\n",
        "```plaintext\n",
        "ai_engineer: 0.8115\n",
        "doctor: 0.8016\n",
        "archaeologist: 0.7592\n",
        "```\n",
        "- Observation : L’écart entre les deux meilleurs experts est très faible (~0.01), ce qui reflète bien la nature interdisciplinaire de la question.\n",
        "- Résultat : Le système a correctement détecté deux domaines pertinents, et a proposé un choix.\n",
        "- Choix utilisateur : ai_engineer\n",
        "- Réponse générée : Précise, complète, bien orientée IA + santé\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 2. Question : Quelles sont les grandes découvertes de l’Égypte antique ?\n",
        "Scores :\n",
        "```plaintext\n",
        "archaeologist: 0.8075\n",
        "doctor: 0.7513\n",
        "ai_engineer: 0.7401\n",
        "```\n",
        "- Observation : L’écart reste significatif, pas de cas interdisciplinaire détecté.\n",
        "- Choix utilisateur : doctor (mauvais choix)\n",
        "- Réponse générée : L’agent médecin ne sait pas répondre → il rejette la requête de l'utilisateur\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 3. Question : Quels sont les effets secondaires du vaccin contre la grippe ?\n",
        "Scores :\n",
        "```plaintext\n",
        "doctor: 0.7559\n",
        "ai_engineer: 0.7251\n",
        "archaeologist: 0.7186\n",
        "```\n",
        "- Observation : Le domaine est clairement médical, pas d’ambiguïté.\n",
        "- Choix utilisateur : doctor\n",
        "- Réponse générée : Complète, précise, adaptée à la question.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 4. Question : Quels sont les avantages de l’apprentissage profond dans le traitement du langage naturel ?\n",
        "Scores :\n",
        "```plaintext\n",
        "ai_engineer: 0.7917\n",
        "doctor: 0.7748\n",
        "archaeologist: 0.7662\n",
        "```\n",
        "- Observation : L’écart est modéré, mais l’IA reste clairement plus pertinente.\n",
        "- Choix utilisateur : auto\n",
        "- Réponse générée : Très adaptée à la question, centrée sur le deep learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "lbFJCPggYDkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💡 Réflexion sur l’impact du routage sémantique\n",
        "\n",
        "###✅ Points forts\n",
        "- Pertinence accrue des réponses\n",
        "\n",
        "- En dirigeant automatiquement les questions vers l’expert le plus adapté, on réduit les erreurs de compréhension du domaine.\n",
        "\n",
        "- L’expert médecin ne répond plus aux questions sur les pyramides, par exemple.\n",
        "\n",
        "- Gestion efficace des cas interdisciplinaires\n",
        "\n",
        "- L’algorithme détecte des cas où plusieurs experts peuvent être légitimes (ex : IA + médecine).\n",
        "\n",
        "- Le système propose un choix à l'utilisateur ou peut fusionner les réponses.\n",
        "\n",
        "- Personnalisation dynamique\n",
        "\n",
        "- L'utilisateur peut ajuster ou valider la sélection, ce qui permet un contrôle sans sacrifier l'automatisation.\n",
        "\n",
        "- Évolutivité naturelle\n",
        "\n",
        "- On peut facilement ajouter de nouveaux experts avec leurs prompts, sans modifier le pipeline.\n",
        "\n",
        "### ⚠️ Limites et pistes d’amélioration\n",
        "\n",
        "- Choix utilisateur incorrect possible\n",
        "\n",
        "- Seuil d’interdisciplinarité fixe :\n",
        "\n",
        "Le seuil 0.01 peut être trop rigide. Une meilleure approche serait d’utiliser un seuil dynamique, ou même de toujours proposer les deux meilleurs scores si les deux sont > 0.78, par exemple.\n",
        "\n",
        "- Fusion de réponses (option avancée) :\n",
        "\n",
        "Pour les cas IA + médecine, il serait utile de générer une réponse collaborative à partir des deux prompts.\n"
      ],
      "metadata": {
        "id": "GXFNXxw-ZNZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✍️ Conclusion\n",
        "Le routage sémantique permet d'améliorer considérablement la précision, la pertinence et la flexibilité des réponses dans un système de question/réponse basé sur des experts virtuels. En intégrant une phase d'affinage et une gestion des cas interdisciplinaires, le système devient à la fois intelligent et interactif — ce qui constitue un pas décisif vers des assistants IA réellement spécialisés et adaptatifs."
      ],
      "metadata": {
        "id": "_OAeaWByZ2Af"
      }
    }
  ]
}